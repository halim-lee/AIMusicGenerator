{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPQ2oTOyrSiA"
   },
   "source": [
    "# Project File - APS360 Team 25\n",
    "Divided into the following section: \n",
    "# \n",
    "1) Library imports\n",
    "2) Data imports\n",
    "3) Model architecture definition\n",
    "4) Training function definition\n",
    "5) Model training\n",
    "6) Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxCkcc2ArSiA"
   },
   "source": [
    "## Library imports \n",
    "(Place all library imports here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qdi1ymBrSiA",
    "outputId": "b3f9649f-15e3-45da-b14a-505714afdad9"
   },
   "outputs": [],
   "source": [
    "#KP - I just added the main ones from the labs.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import time # Tracking model training time.\n",
    "\n",
    "# Install mido for Data importing\n",
    "'''\n",
    "Pip install for COLAB:\n",
    "'''\n",
    "# !pip install mido;\n",
    "import mido\n",
    "from mido import MidiFile, Message, MidiTrack, MetaMessage\n",
    "import os\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3b7uuDcrqIj",
    "outputId": "eaf04f40-d5ff-40f2-b395-6854c3e36e8e"
   },
   "outputs": [],
   "source": [
    "#Set working directory if required:\n",
    "''' For LOCAL:\n",
    "'''\n",
    "os.chdir('D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot') #Sets current working directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "755DNUDOrSiB"
   },
   "source": [
    "## Data imports\n",
    "#### MIDI reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UGf_y0zxrSiB"
   },
   "outputs": [],
   "source": [
    "def CountTracks(directory):          #Count files and tracks in folder\n",
    "    trackCount = 0\n",
    "    fileCount = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".midi\"):\n",
    "            fileCount += 1\n",
    "            midiDir = MidiFile(directory+\"/\"+file)\n",
    "            for track in midiDir.tracks:\n",
    "                trackCount += 1\n",
    "    print(fileCount+\" files\")\n",
    "    print(trackCount+\" tracks\")\n",
    "\n",
    "    \n",
    "def PrintMessages(mid):                # print midi messages\n",
    "    for i, track in enumerate(mid.tracks):\n",
    "        print('Track {}: {}'.format(i, track.name))\n",
    "        for msg in track:\n",
    "            print(msg)\n",
    "\n",
    "            \n",
    "def PrintSomeMessages(mid):             #print first 200 midi messages\n",
    "    track = mid.tracks[1]\n",
    "    for i,msg in enumerate(track):\n",
    "        if i < 200:\n",
    "            print(msg)\n",
    "            \n",
    "def PrintMetaMessages(mid):             #print fmeta messages\n",
    "    track = mid.tracks[0]\n",
    "    for i,msg in enumerate(track):\n",
    "        print(msg)\n",
    "\n",
    "def cleanupMessages(mid):              #removes non-note messages by force\n",
    "    track = mid.tracks[1]\n",
    "    track2 = []\n",
    "    for msg in track:\n",
    "        if msg.type == \"note_on\":\n",
    "            track2.append(msg)\n",
    "    mid.tracks[1] = track2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UT23qIIrSiB"
   },
   "source": [
    "#### MIDI to Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zsvL0x4krSiB"
   },
   "outputs": [],
   "source": [
    "def Midi2NumpyNoSustain(mid, track0=1):                                #converts to numpy array removing non-note messages\n",
    "    track = mid.tracks[track0]                           #0th track only contains meta-messages, all notes on 1st track\n",
    "    notes = np.empty([0,4])\n",
    "    time = 0\n",
    "    for msg in track:\n",
    "        if msg.type == \"note_on\":                   # only count \"note\" messages - other inputs i.e. foot pedals are ignored\n",
    "            notes = np.append(notes,np.array([[msg.note, msg.velocity, msg.time + time, 0]]),axis=0)         # (note, velocity, time, sustain)\n",
    "            time = 0\n",
    "        elif msg.type == \"note_off\":\n",
    "            notes = np.append(notes,np.array([[msg.note, 0, msg.time + time, 0]]),axis=0)         # (note, velocity, time, sustain)\n",
    "            time = 0        \n",
    "        else:\n",
    "            time += msg.time                        #adjust time when removing other messages\n",
    "    return notes\n",
    "\n",
    "\n",
    "def NumpyGetSustain(note):\n",
    "    notes = np.copy(note)\n",
    "    for i, msg in enumerate(notes):\n",
    "        if msg[1] > 0:                            # if velocity is not 0\n",
    "            j = 1\n",
    "            sustain = 0\n",
    "            while msg[0] != notes[i+j][0]:        # while note values are different\n",
    "                sustain += notes[i+j][2]\n",
    "                j += 1                            #search for next message with same note i.e. message telling that note was released\n",
    "            notes[i,3] = sustain + notes[i+j][2]\n",
    "    time = 0\n",
    "    for i, msg in enumerate(notes):\n",
    "        if msg[1] > 0:\n",
    "            notes[i,2] += time\n",
    "            time = 0\n",
    "        else:\n",
    "            time += msg[2]                        #adjust time\n",
    "    notes = notes[notes[:,1] > 0]                 #filter for notes with positive velocities (note presses)\n",
    "    return notes\n",
    "\n",
    "def NumpyNormalize(note, oneHot=False, full=False):                         #normalize all values to 0-1\n",
    "    notes = np.copy(note)\n",
    "    \n",
    "    if oneHot:\n",
    "        if full:\n",
    "            notes[:,88] /= 128\n",
    "            notes[:,89] /= 40000\n",
    "            notes[:,90] /= 40000\n",
    "        else:\n",
    "            notes[:,12] /= 11\n",
    "            notes[:,13] /= 128\n",
    "            notes[:,14] /= 40000\n",
    "            notes[:,15] /= 40000\n",
    "    else:\n",
    "        notes[:,0] /= 128\n",
    "        notes[:,1] /= 128\n",
    "        notes[:,2] /= 40000\n",
    "        notes[:,3] /= 40000       \n",
    "    return notes\n",
    "\n",
    "def NumpyOneHot(note):\n",
    "    notes = np.copy(note)\n",
    "    oneHot = np.zeros([len(notes),16])\n",
    "    oneHot[:, 13:] = notes[:, 1:]\n",
    "    names = notes[:,0]\n",
    "    namesOct = names%12\n",
    "    oneHot[:,12] = (names-(namesOct))/12\n",
    "    \n",
    "    for i, name in enumerate(namesOct):\n",
    "        oneHot[i,name.astype(int)] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "def NumpyNotesOneHot(note):\n",
    "    notes = np.copy(note)\n",
    "    \n",
    "    oneHot = np.zeros([len(notes),91])\n",
    "    oneHot[:, 88:] = notes[:, 1:]\n",
    "    names = notes[:,0]-21\n",
    "    #namesOct = names%12\n",
    "    #oneHot[:,12] = (names-(namesOct))/12\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        oneHot[i,name.astype(int)] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "def Midi2Numpy(path, oneHot=False, track0=1): # full midi to numpy conversion\n",
    "    mid = MidiFile(path)\n",
    "    notes = Midi2NumpyNoSustain(mid, track0=track0)\n",
    "    cleanNotes = NumpyGetSustain(notes)\n",
    "    \n",
    "    if oneHot:\n",
    "        cleanNotes = NumpyOneHot(cleanNotes)\n",
    "    \n",
    "    normNotes = NumpyNormalize(cleanNotes, oneHot=oneHot)\n",
    "    return normNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ3C4NpgrSiB"
   },
   "source": [
    "#### Numpy to MIDI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FcyTTyH-rSiB"
   },
   "outputs": [],
   "source": [
    "def NumpyDenormalize(note): # interpret all values from 0-1 to normal values\n",
    "    notes = np.copy(note)    \n",
    "    if notes.shape[1] == 16: # if encode as one-hot\n",
    "        notes[:,12] *= 11 # octave\n",
    "        notes[:,13] *= 128 # vel\n",
    "        notes[:,14] *= 40000 # time\n",
    "        notes[:,15] *= 40000 # sustain\n",
    "        \n",
    "        notes = NumpyEncode(notes) #encode back as original 4-variable format\n",
    "        \n",
    "    elif notes.shape[1] == 91: # if encoded as one-hot w/o octave\n",
    "        notes[:,88] *= 128\n",
    "        notes[:,89] *= 40000\n",
    "        notes[:,90] *= 40000\n",
    "        \n",
    "        #print(notes)\n",
    "        \n",
    "        notes = NumpyEncodeNotes(notes)\n",
    "        \n",
    "    else:\n",
    "        notes[:,0] *= 128\n",
    "        notes[:,1] *= 128\n",
    "        notes[:,2] *= 40000\n",
    "        notes[:,3] *= 40000       \n",
    "    return notes.astype(int)\n",
    "\n",
    "def NumpyEncode(note): # convert back from one-hot encoding\n",
    "    notes = np.copy(note)    \n",
    "    encoded = np.zeros([len(notes),4]) # create array   \n",
    "    encoded[:, 1:] = notes[:, 13:] # set vel/time/sustain\n",
    "    encoded[:, 0] = notes[:,12]*12 # add octave value\n",
    "    \n",
    "    for i in range(len(notes)):\n",
    "        encoded[i,0] += np.argmax(notes[i,:12])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def NumpyEncodeNotes(note): # convert back from one-hot encoding\n",
    "    notes = np.copy(note)    \n",
    "    encoded = np.zeros([len(notes),4])\n",
    "    \n",
    "    encoded[:, 1:] = notes[:, 88:] # set vel/time/sustain\n",
    "    \n",
    "    for i in range(len(notes)):\n",
    "        encoded[i,0] += np.argmax(notes[i,:88])+21\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def NumpySequence(notes): # put all notes into a \"timeline\" i.e.: time values of [10, 20, 10, 30] become [10, 30, 40, 70]\n",
    "    sequenced = np.copy(notes)                      # this allows us to easily add vel=0 notes in any order since we can later sort them by time\n",
    "    for i, msg in enumerate(sequenced):\n",
    "        if i > 0:\n",
    "            sequenced[i,2] += sequenced[i-1,2]\n",
    "    return sequenced\n",
    "\n",
    "def NumpyAddOffNotes(sequenced): # add vel=0 notes from sustain into sequenced timeline\n",
    "    withOff = np.copy(sequenced)\n",
    "    for msg in sequenced:\n",
    "        offNote = np.array([[msg[0], 0, msg[2] + msg[3], 0]])\n",
    "        withOff = np.append(withOff, offNote, axis=0)\n",
    "    #withOff = np.sort(withOff,axis=0)\n",
    "    withOff = withOff[withOff[:,2].argsort()] # sort by time\n",
    "    return withOff\n",
    "\n",
    "def NumpyUnsequence(notes): # revert time value to \"time since last message\"\n",
    "    unsequenced = np.copy(notes)\n",
    "    for i, msg in reversed(list(enumerate(unsequenced))):\n",
    "        unsequenced[i,3] = 0\n",
    "        if i > 0:\n",
    "            unsequenced[i,2] -= unsequenced[i-1,2]\n",
    "    return unsequenced\n",
    "\n",
    "def Numpy2MidiDirect(array):    #make MIDI object from numpy\n",
    "    #Start with initializing a new Mido Track:\n",
    "    mid = MidiFile()\n",
    "    track0 = MidiTrack()\n",
    "    track1 = MidiTrack()\n",
    "    \n",
    "    track0.append(MetaMessage('set_tempo', tempo=500000, time=0)) #MetaMessages not necessary but are present in used files\n",
    "    track0.append(MetaMessage('time_signature', numerator=4, denominator=4, clocks_per_click=24, notated_32nd_notes_per_beat=8, time=0))\n",
    "    track0.append(MetaMessage('end_of_track', time=1))\n",
    "    \n",
    "    track1.append(Message('program_change', channel=0, program=0, time=0))\n",
    "    \n",
    "    for i,note in enumerate(array):         # Get the index and the note. Array must be int array\n",
    "        j = 1\n",
    "        track1.append(Message('note_on',note = array[i,0], velocity = array[i,1],time = array[i,2])) # Add the note to the track.\n",
    "\n",
    "    mid.tracks.append(track0)\n",
    "    mid.tracks.append(track1)\n",
    "    return mid\n",
    "\n",
    "def Numpy2Midi(notes, name): # full numpy to midi conversion, saving result to [name].midi\n",
    "    denorm = NumpyDenormalize(notes)\n",
    "    seq = NumpySequence(denorm)\n",
    "    off = NumpyAddOffNotes(seq)\n",
    "    unseq = NumpyUnsequence(off)\n",
    "    mid = Numpy2MidiDirect(unseq)\n",
    "    mid.save(name + \".midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OCUYyBiIrSiB"
   },
   "outputs": [],
   "source": [
    "def splitTrack(midiFileName, inputDir, outputDir):\n",
    "    mid = MidiFile(inputDir+midiFileName+\".midi\")\n",
    "    for t, track in enumerate(mid.tracks):\n",
    "        program, channel = findInstrument(track)        \n",
    "        if len(track) < 200 or program == -1:\n",
    "            continue\n",
    "        trackMidi = copy.copy(mid)\n",
    "        indices = [t]\n",
    "        trackMidi.tracks = [trackMidi.tracks[x] for x in indices]\n",
    "        #trackMidi.tracks.append(track)\n",
    "        trackMidi.save(outputDir + midiFileName + \"_prog{:0>3d}_chan{:0>2d}.midi\".format(program,channel))\n",
    "\n",
    "def splitAllTracks(inputDir, outputDir, first=0): # very bad code -  doesnt remove the .midi in the middle lol\n",
    "    for i,f in enumerate(os.listdir(inputDir)):\n",
    "        if i >= first:\n",
    "            if len(f) <7:\n",
    "                continue\n",
    "            splitTrack(f[:7],inputDir,outputDir)\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                print(f)\n",
    "        \n",
    "def findInstrument(track):\n",
    "    count = 0\n",
    "    infoMsg = 0\n",
    "    \n",
    "    for msg in track:\n",
    "        if msg.type == \"program_change\":\n",
    "            count += 1\n",
    "            infoMsg = msg\n",
    "    \n",
    "    if count >= 1:\n",
    "        if infoMsg.channel == 9 or count == 1:\n",
    "            return infoMsg.program, infoMsg.channel\n",
    "        else: \n",
    "            return -1, -1\n",
    "    else:\n",
    "        return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iP4lojMgrSiB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#splitAllTracks(\"data/jazz/\",\"data/jazz/tracks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTtFiHvFrSiB"
   },
   "source": [
    "#### Generatng tensor dataset from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vxt1UBhHrSiB"
   },
   "outputs": [],
   "source": [
    "def Numpy2Dataset(notes, tgt, num=100,skip=200): # make list of sumpy arrays\n",
    "    samples = []\n",
    "    targets = []\n",
    "    i = 0\n",
    "    while i+num <= len(notes):\n",
    "        samples.append(notes[i:i+num])\n",
    "        targets.append(tgt)\n",
    "        i += skip\n",
    "    return samples, targets\n",
    "\n",
    "def SampleAllNumpy(dataPath): # generate samples from all saved CSVs\n",
    "    allSamples = []\n",
    "    allTargets = []\n",
    "\n",
    "    for i,f in enumerate(os.listdir(dataPath)):\n",
    "        notes = np.genfromtxt(dataPath+f, delimiter=',')\n",
    "        tgt = GetGroup(f)\n",
    "                             \n",
    "        samples, targets = Numpy2Dataset(notes, tgt)\n",
    "        allSamples += samples\n",
    "        allTargets += targets\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    random.seed(0)\n",
    "    random.shuffle(allSamples)\n",
    "    random.seed(0)\n",
    "    random.shuffle(allTargets) # shuffle samples and targets in the exact same way\n",
    "    \n",
    "    return allSamples, allTargets\n",
    "\n",
    "def SaveSamplesTensor(samples, targets, outputPath, name=\"Notes_Dataset\"): # save tensor\n",
    "    tens = torch.Tensor(samples)\n",
    "    targ = torch.Tensor(targets)\n",
    "    dataset = TensorDataset(tens,targ)\n",
    "    torch.save(dataset, outputPath+name+\".pt\")\n",
    "    return tens   \n",
    "\n",
    "def SaveAllSamples(dataPath, outputPath, name=\"Notes_Dataset\"): # save dataset tensor\n",
    "    samples, targets = SampleAllNumpy(dataPath)\n",
    "    SaveSamplesTensor(samples, targets, outputPath, name)\n",
    "    \n",
    "def SplitSamples(dataset, ranges):\n",
    "    torch.maual_seed(0)\n",
    "    trainData, valData, testData = random_split(oneHotDataset, splitRange)\n",
    "    return trainData, valData, testData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DGM_cpI2rSiB"
   },
   "outputs": [],
   "source": [
    "def ProgramGroup(program, channel):\n",
    "    if channel == 9:\n",
    "        return 0 # drums\n",
    "    if program < 8:\n",
    "        return 1 # piano\n",
    "    if program < 16:\n",
    "        return 8 # pitched percussion\n",
    "    if program < 24:\n",
    "        return 7 # organ\n",
    "    if program < 32:\n",
    "        return 3 # guitar\n",
    "    if program < 40:\n",
    "        return 2 # bass\n",
    "    if program < 48:\n",
    "        return 9 # string\n",
    "    if program < 56:\n",
    "        return 10 # ensemble\n",
    "    if program < 64:\n",
    "        return 4 # brass \n",
    "    if program < 72:\n",
    "        return 5 # reed\n",
    "    if program < 80:\n",
    "        return 11 # pipe\n",
    "    if program < 88:\n",
    "        return 6 # synth lead\n",
    "    if program < 96:\n",
    "        return 12 # synth pad\n",
    "    if program < 104:\n",
    "        return 13 # synth efects\n",
    "    if program < 112:\n",
    "        return 14 # ethnic\n",
    "    if program < 120:\n",
    "        return 15 # percusson\n",
    "    else:\n",
    "        return 16 # other \n",
    "    \n",
    "def GetProgram(name):\n",
    "    prog = name[12:15]\n",
    "    chan = name[20:22]\n",
    "    return int(prog), int(chan)\n",
    "\n",
    "def GetGroup(name):\n",
    "    prog, chan = GetProgram(name)\n",
    "    group = ProgramGroup(prog, chan)\n",
    "    return(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcsmXvAarSiB"
   },
   "source": [
    "#### Bulk data conversion code - COMMENT OUT IF NOT IN USE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vKGMTqwurSiB"
   },
   "outputs": [],
   "source": [
    "#SaveAllSamples(\"data/jazz/tracks_numpy/train/\",\"data/\",\"trainDataClass\") #save all into tensor\n",
    "#SaveAllSamples(\"data/jazz/tracks_numpy/val/\",\"data/\",\"valDataClass\") #save all into tensor\n",
    "#SaveAllSamples(\"data/jazz/tracks_numpy/test/\",\"data/\",\"testDataClass\") #save all into tensor\n",
    "\n",
    "# oneHotDataset = torch.load(\"data/onehot_data/onehot.pt\")\n",
    "\n",
    "# splitProp = np.array([0.9, 0.05 , 0.05]) # 90/5/5% split\n",
    "# splitRange = len(oneHotDataset)*splitProp\n",
    "# splitRange[0] += 1                           # so numbers add up\n",
    "# splitRange = splitRange.astype(int)\n",
    "\n",
    "# length = len(oneHotDataset)\n",
    "# a = 0.9*length\n",
    "# a = int(a)\n",
    "\n",
    "# b = 0.95*length\n",
    "# b = int(b)\n",
    "\n",
    "# tiny = oneHotDataset[:30]\n",
    "# train = oneHotDataset[:a]\n",
    "# val = oneHotDataset[a:b]\n",
    "# test = oneHotDataset[b:]\n",
    "\n",
    "# tinyData = TensorDataset(tiny[0].clone(), tiny[1].clone())\n",
    "# trainData = TensorDataset(train[0].clone(),train[1].clone())\n",
    "# valData = TensorDataset(val[0].clone(),val[1].clone())\n",
    "# testData = TensorDataset(test[0].clone(),test[1].clone())\n",
    "\n",
    "# torch.save(tinyData,\"data/onehot_data/tinyDataOneHot.pt\")\n",
    "# torch.save(trainData,\"data/onehot_data/trainDataOneHot.pt\")\n",
    "# torch.save(valData,\"data/onehot_data/valDataOneHot.pt\")\n",
    "# torch.save(testData,\"data/onehot_data/testDataOneHot.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ssp85azWrSiC"
   },
   "outputs": [],
   "source": [
    "#a = torch.load(\"data/va.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "63avpbMHrSiC",
    "outputId": "38903c44-8edc-4f0b-9005-213cafdb812e"
   },
   "outputs": [],
   "source": [
    "# for img, label in iter(a):\n",
    "#     #print(img)\n",
    "#     print(label)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dHJs765krSiC"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: COMMENT OUT IF NOT IN USE TO AVOID ACCIDENTS!!!!!!!\n",
    "\n",
    "# Getting CSVs from MIDI data and processed data from CSVs\n",
    "# Processed MIDI does not contain program messages and so are a good measure of what output SHOULD look like in a perfect world\n",
    "\n",
    "# dataPath = \"data/jazz/tracks/\"\n",
    "# outputPath = \"data/jazz/tracks_numpy/\"\n",
    "# #processedPath = \"data/MIDI_files_processed/\"\n",
    "\n",
    "# for i,f in enumerate(os.listdir(dataPath)):\n",
    "#     if i > 2700:\n",
    "#         notes = Midi2Numpy(dataPath+f,track0=0)\n",
    "#         np.savetxt(outputPath + f[:22]+\".csv\",notes,delimiter=\",\")\n",
    "#     #Numpy2Midi(notes, processedPath + \"MIDI_{:04d}\".format(i))\n",
    "#     #break\n",
    "    \n",
    "#     if i % 200 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GmrcKT1-rSiC"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: COMMENT OUT IF NOT IN USE TO AVOID ACCIDENTS!!!!!!!\n",
    "\n",
    "# Getting CSVs from MIDI data and processed data from CSVs\n",
    "# Processed MIDI does not contain program messages and so are a good measure of what output SHOULD look like in a perfect world\n",
    "\n",
    "# dataPath = \"data/jazz/tracks/\"\n",
    "# outputPath = \"data/jazz/tracks_numpy/\"\n",
    "# #processedPath = \"data/MIDI_files_processed/\"\n",
    "\n",
    "# for i,f in enumerate(os.listdir(dataPath)):\n",
    "#     if i == 47:\n",
    "#         print(f)\n",
    "#         testMidi = MidiFile(dataPath+f)\n",
    "#         #PrintMessages(testMidi)\n",
    "#         print(GetGroup(f))\n",
    "#     #Numpy2Midi(notes, processedPath + \"MIDI_{:04d}\".format(i))\n",
    "#     #break\n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "r4lW9eKUrSiC"
   },
   "outputs": [],
   "source": [
    "# dataPath = \"data/numpy_files/\"  # one-hot encoding on CSVs\n",
    "# outputPath = \"data/numpy_full/\"\n",
    "\n",
    "# for i,f in enumerate(os.listdir(dataPath)):\n",
    "#     notes = np.genfromtxt(dataPath+f, delimiter=',')\n",
    "#     notes = NumpyDenormalize(notes)\n",
    "#     notes = NumpyNotesOneHot(notes)\n",
    "#     notes = NumpyNormalize(notes, oneHot=True, full=True)\n",
    "#     np.savetxt(outputPath + \"MIDI_{:04d}.csv\".format(i),notes,delimiter=\",\")\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0fsM7MtrSiC"
   },
   "source": [
    "## Baseline Model Code\n",
    "#### getting available notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cqmkQTqrSiC"
   },
   "source": [
    "## Model architecture definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiA0KsAkrSiC"
   },
   "source": [
    "Set the hyperparameters below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4gpkIALrSiC",
    "outputId": "48c295e0-fcac-4b7a-aa4f-536bf2bef54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class created succesfully\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden, n_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.name = \"LSTMClassifier\"\n",
    "        self.n_features, self.n_classes, self.n_hidden, self.n_layers = n_features, n_classes, n_hidden, n_layers, \n",
    "        self.lstm = nn.LSTM(n_features, n_hidden, n_layers, batch_first=True,dropout = 0.3)\n",
    "        self.fc = torch.nn.Linear(2*n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagate the RNN\n",
    "        h0 = torch.zeros(self.n_layers,x.size(0),self.n_hidden)\n",
    "        c0 = torch.zeros(self.n_layers,x.size(0),self.n_hidden)\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            c0 = c0.cuda()\n",
    "            h0 = h0.cuda()\n",
    "        out, _ = self.lstm(x)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = torch.cat([torch.max(out, dim=1)[0], \n",
    "                    torch.mean(out, dim=1)], dim=1) #Combine max and mean\n",
    "        output = self.fc(out)\n",
    "        return output\n",
    "\n",
    "print('Model class created succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D2idmcbrSiD"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "G5ENRYH9rSiD"
   },
   "outputs": [],
   "source": [
    "#To help us save the model easier...\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GLOGDWmrubIm"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader): #Accuracy on note selection...\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    model.eval()\n",
    "    for sample in data_loader:\n",
    "        inputs, labels = sample\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "        output = model(inputs)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        if i%100 == 0:\n",
    "            print(pred)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += len(labels)\n",
    "        i += 1\n",
    "    return (correct / total)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Fpju80lNTuoE"
   },
   "outputs": [],
   "source": [
    "# Training Curve\n",
    "def plot_training_curve(accLoss, train, val):\n",
    "    \"\"\" Plots the training curve for a model's train/validation loss or accuracy.\n",
    "\n",
    "    Args:\n",
    "        accLoss: \"Accuracy\" or \"Loss\"\n",
    "        train: train data\n",
    "        val: validation data\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    titleStr = \"Train vs Validation \" + accLoss\n",
    "    plt.title(titleStr)\n",
    "    n = len(train) # number of epochs\n",
    "    plt.plot(range(1,n+1), train, label=\"Train\")\n",
    "    if val != False:\n",
    "        \n",
    "        plt.plot(range(1,n+1), val, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(accLoss)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8KDJooOSVCTi"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset, num_epochs=5, batch_size=64, learning_rate=1e-3,model_name = 'model'):\n",
    "    torch.manual_seed(1000) #Fixed. Make sure we use this throughout...\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, weight_decay=0.001)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    start_time=time.time() #Start of training\n",
    "\n",
    "    train_losses, train_accs, val_accs = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "#             print('labels: ',labels)\n",
    "            model.train()\n",
    "            \n",
    "            out = model(inputs)             # forward pass\n",
    "#             print(out.shape)\n",
    "            loss = criterion(out, labels) # compute the total loss\n",
    "\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()              # make the updates for each parameter\n",
    "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "\n",
    "            loss = float(loss)/batch_size             # compute *average* loss\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader)\n",
    "#         val_loss = get_loss(model, val_loader, criterion)\n",
    "        val_acc =get_accuracy(model, val_loader)\n",
    "\n",
    "        print('Epoch: {} - Train loss: {:.4f}, Train accuracy: {:.2f}%, Validation accuracy: {:.2f}%'.format(\n",
    "                            epoch+1, float(loss), float(train_acc), float(val_acc)))\n",
    "        \n",
    "        train_losses.append(loss)             # compute *average* loss\n",
    "        train_accs.append(train_acc)\n",
    "#         val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    #Save the file current model with the weights\n",
    "    model_path = get_model_name(model.name, batch_size, learning_rate, num_epochs)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    end_time= time.time()\n",
    "    torch.save(model.state_dict(), model_name+\".pt\")\n",
    "    return train_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U5q-SKT-rSiD"
   },
   "outputs": [],
   "source": [
    "def get_class_binary(dataset,label_1,label_2):\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "\n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(label == label_1, label == label_2))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(label == label_1, label == label_2))]\n",
    "\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_3(dataset,label_1,label_2,label_3): #For any 3 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_9(dataset,label_1,label_2,label_3,label_4,label_5,label_6,label_7,label_8,label_9): #For any 9 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6),label==label_7),label==label_8),label==label_9))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6),label==label_7),label==label_8),label==label_9))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_8(dataset,label_1,label_2,label_3,label_4,label_5,label_6,label_7,label_8): #For any 8 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6),label==label_7),label==label_8))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6),label==label_7),label==label_8))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_7(dataset,label_1,label_2,label_3,label_4,label_5,label_6,label_7): #For any 7 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6),label==label_7))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6),label==label_7))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_6(dataset,label_1,label_2,label_3,label_4,label_5,label_6): #For any 6 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5),label==label_6))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_5(dataset,label_1,label_2,label_3,label_4,label_5): #For any 5 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4),label==label_5))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_4(dataset,label_1,label_2,label_3,label_4): #For any 4 classes.\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or((label == label_1), (label == label_2)), (label == label_3)),label==label_4))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(dataset):\n",
    "    list_data = list(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for sample in list_data:\n",
    "        data.append((sample[0]).detach().numpy())\n",
    "        label.append((sample[1]).detach().numpy())\n",
    "    \n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "#     print(label)\n",
    "    # Grab data that are classified as 0 or 1,2,3,4,5,6,7,8,9... Other classes have 0 samples.\n",
    "    class_0_1_data = data[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == 0), (label == 1)), (label == 2)),label==4),label==5),label==6),label==7),label==8),label==9),label==3))]\n",
    "    class_0_1_label = label[np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or(np.logical_or((label == 0), (label == 1)), (label == 2)),label==4),label==5),label==6),label==7),label==8),label==9),label==3))]\n",
    "    print(class_0_1_label)\n",
    "    # Prevent type errors\n",
    "    out_dataset = TensorDataset(torch.tensor(class_0_1_data).float(), torch.tensor(class_0_1_label).long())\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DyDP2xejwnXc"
   },
   "outputs": [],
   "source": [
    "# Load saved tensor dataset\n",
    "'''\n",
    "For colab:\n",
    "'''\n",
    "# train_dataset = torch.load(r'/content/gdrive/My Drive/APS360/Project/trainDataClass.pt')\n",
    "# val_dataset = torch.load(r'/content/gdrive/My Drive/APS360/Project/valDataClass.pt')\n",
    "# test_dataset = torch.load(r'/content/gdrive/My Drive/APS360/Project/testDataClass.pt')\n",
    "'''\n",
    "For local: (@Kevin)\n",
    "'''\n",
    "'''\n",
    "Normal:\n",
    "'''\n",
    "# train_dataset = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot\\dataset\\ClassData (1)\\normal\\trainDataClass.pt')\n",
    "# val_dataset = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot\\dataset\\ClassData (1)\\normal\\valDataClass.pt')\n",
    "# test_dataset = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot\\dataset\\ClassData (1)\\normal\\testDataClass.pt')\n",
    "\n",
    "'''\n",
    "One hot:\n",
    "'''\n",
    "train_dataset = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot\\dataset\\ClassData (1)\\onehot\\trainDataClassOnehot.pt')\n",
    "val_dataset = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot\\dataset\\ClassData (1)\\onehot\\valDataClassOnehot.pt')\n",
    "test_dataset = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot\\dataset\\ClassData (1)\\onehot\\testDataClassOnehot.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print stats from the dataset\n",
    "# for imgs, labels in torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True):\n",
    "# #     print(labels)\n",
    "#     labels = labels\n",
    "\n",
    "# hist_var = np.zeros((8))\n",
    "# for i in range(len(labels)):\n",
    "# #     print(labels[i].numpy())\n",
    "#     #Count each class samples...\n",
    "#     for j in range(0,8):\n",
    "#         if labels[i] == j:\n",
    "#             hist_var[j] += 1\n",
    "# print(hist_var) #Good visual for the demo! (see plot below :) )\n",
    "\n",
    "\n",
    "# plt.plot(np.linspace(0,7,8),hist_var,\"o\")\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.xlabel('Class')\n",
    "# plt.title('TrainData Class Distributions')\n",
    "# plt.show()\n",
    "\n",
    "# for imgs, labels in torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True):\n",
    "# #     print(labels)\n",
    "#     labels = labels\n",
    "\n",
    "# hist_var = np.zeros((8))\n",
    "# for i in range(len(labels)):\n",
    "# #     print(labels[i].numpy())\n",
    "#     #Count each class samples...\n",
    "#     for j in range(0,8):\n",
    "#         if labels[i] == j:\n",
    "#             hist_var[j] += 1\n",
    "# print(hist_var) #Good visual for the demo! (see plot below :) )\n",
    "\n",
    "\n",
    "# plt.plot(np.linspace(0,7,8),hist_var,\"o\")\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.xlabel('Class')\n",
    "# plt.title('valData Class Distributions')\n",
    "# plt.show()\n",
    "\n",
    "# for imgs, labels in torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True):\n",
    "# #     print(labels)\n",
    "#     labels = labels\n",
    "\n",
    "# hist_var = np.zeros((8))\n",
    "# for i in range(len(labels)):\n",
    "# #     print(labels[i].numpy())\n",
    "#     #Count each class samples...\n",
    "#     for j in range(0,8):\n",
    "#         if labels[i] == j:\n",
    "#             hist_var[j] += 1\n",
    "# print(hist_var) #Good visual for the demo! (see plot below :) )\n",
    "\n",
    "\n",
    "# plt.plot(np.linspace(0,7,8),hist_var,\"o\")\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.xlabel('Class')\n",
    "# plt.title('testData Class Distributions')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WEJ4OXb67IUX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 0. 5. ... 5. 4. 3.]\n",
      "[6. 2. 4. ... 0. 2. 3.]\n",
      "[1. 0. 1. ... 0. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = get_class_3(train_dataset,0,1,2)\n",
    "# val_dataset = get_class_3(val_dataset,0,1,2)\n",
    "# test_dataset = get_class_3(test_dataset,0,1,2)\n",
    "\n",
    "#Run the 2,5,6\n",
    "train_dataset = get_class_7(train_dataset,0,1,2,3,4,5,6)\n",
    "val_dataset = get_class_7(val_dataset,0,1,2,3,4,5,6)\n",
    "test_dataset = get_class_7(test_dataset,0,1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Hr_c4ReHx191"
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# For LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html #torch.nn.LSTM \n",
    "# Hyperparameters\n",
    "# 8 features overfits. Trying with dropout.\n",
    "N_FEATURES = 131\n",
    "N_CLASSES = 7\n",
    "N_HIDDEN = 32\n",
    "N_LAYERS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 1e-3\n",
    "model = LSTMClassifier(n_features=N_FEATURES, n_classes = N_CLASSES, n_hidden=N_HIDDEN, n_layers=N_LAYERS)\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0RWdIsr-oLi",
    "outputId": "f98f71c5-86c2-44b9-a35f-326e6da2b249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [3],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2]], device='cuda:0')\n",
      "Epoch: 1 - Train loss: 0.0108, Train accuracy: 42.86%, Validation accuracy: 43.89%\n",
      "tensor([[5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5]], device='cuda:0')\n",
      "tensor([[5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [3],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4]], device='cuda:0')\n",
      "Epoch: 2 - Train loss: 0.0103, Train accuracy: 55.50%, Validation accuracy: 56.11%\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [6],\n",
      "        [3],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2]], device='cuda:0')\n",
      "tensor([[5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4]], device='cuda:0')\n",
      "tensor([[6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5]], device='cuda:0')\n",
      "tensor([[3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4]], device='cuda:0')\n",
      "Epoch: 3 - Train loss: 0.0107, Train accuracy: 62.72%, Validation accuracy: 57.35%\n",
      "tensor([[4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [3],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[3],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4]], device='cuda:0')\n",
      "tensor([[4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0')\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [4],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2]], device='cuda:0')\n",
      "Epoch: 4 - Train loss: 0.0066, Train accuracy: 65.44%, Validation accuracy: 57.95%\n",
      "tensor([[0],\n",
      "        [6],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3]], device='cuda:0')\n",
      "tensor([[1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2]], device='cuda:0')\n",
      "tensor([[1],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5]], device='cuda:0')\n",
      "tensor([[6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [4],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3],\n",
      "        [5],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4]], device='cuda:0')\n",
      "Epoch: 5 - Train loss: 0.0085, Train accuracy: 66.34%, Validation accuracy: 57.19%\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, val_accs = train(model, train_dataset, val_dataset, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,model_name='8_class_model_onehot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Ue6n6_07WJFW",
    "outputId": "2489c0d7-1818-496f-889f-9cc0f885a203"
   },
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_training_curve(\"Loss\", train_losses,False)\n",
    "\n",
    "# Plot accuracy curves\n",
    "plot_training_curve(\"Accuracy\", train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "for i, data in enumerate(train_loader):\n",
    "    inputs, y_true = data\n",
    "\n",
    "#             print('labels: ',labels)\n",
    "    output = model(inputs)             # forward pass\n",
    "    y_pred = output.max(1, keepdim=True)[1]\n",
    "print(y_pred.shape)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_true = y_true.detach().numpy()\n",
    "# print(y_pred)\n",
    "print(\"confusion matrix from trainin set\") #https://codeyarns.com/tech/2014-10-24-how-to-create-a-confusion-matrix-plot-using-matplotlib.html\n",
    "m = confusion_matrix(y_true,y_pred,normalize = 'true')\n",
    "plt.matshow(m)\n",
    "plt.colorbar()\n",
    "\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_true = data\n",
    "\n",
    "#             print('labels: ',labels)\n",
    "    output = model(inputs)             # forward pass\n",
    "    y_pred = output.max(1, keepdim=True)[1]\n",
    "print(y_pred.shape)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_true = y_true.detach().numpy()\n",
    "# print(y_pred)\n",
    "print(\"confusion matrix from validation set\") #https://codeyarns.com/tech/2014-10-24-how-to-create-a-confusion-matrix-plot-using-matplotlib.html\n",
    "m = confusion_matrix(y_true,y_pred,normalize = 'true')\n",
    "plt.matshow(m)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtmDqDHmrSiD"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOIt01mprSiD",
    "outputId": "d2e266f1-d4b8-42d3-b679-d90c77bda27a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Only run at the end...\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# model = model.cuda()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# test_acc = get_accuracy(model, test_loader)\n",
    "# test_loss = get_loss(model, test_loader, criterion)\n",
    "# model = model.cpu()\n",
    "# model.eval()\n",
    "# print((\"Test Loss: {:.4f}\").format(float(test_loss)))\n",
    "# print((\"Test Accuracy: {:.2f}%\").format(float(test_acc)))\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "# for i, data in enumerate(test_loader):\n",
    "#     inputs, y_true = data\n",
    "\n",
    "# #             print('labels: ',labels)\n",
    "#     output = model(inputs)             # forward pass\n",
    "#     y_pred = output.max(1, keepdim=True)[1]\n",
    "# print(y_pred.shape)\n",
    "# y_pred = y_pred.detach().numpy()\n",
    "# y_true = y_true.detach().numpy()\n",
    "# # print(y_pred)\n",
    "# print(\"confusion matrix from validation set\") #https://codeyarns.com/tech/2014-10-24-how-to-create-a-confusion-matrix-plot-using-matplotlib.html\n",
    "# m = confusion_matrix(y_true,y_pred,normalize = 'true')\n",
    "# plt.matshow(m)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMClassifier(n_features=N_FEATURES, n_classes = N_CLASSES, n_hidden=N_HIDDEN, n_layers=N_LAYERS)\n",
    "\n",
    "# model.load_state_dict(torch.load('D:/engsci/year 3/CLASS/APS360/project_pivot/7_class_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Project_workbook_inst_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
