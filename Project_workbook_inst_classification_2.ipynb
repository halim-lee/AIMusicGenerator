{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPQ2oTOyrSiA"
   },
   "source": [
    "# Project File - APS360 Team 25\n",
    "Divided into the following section: \n",
    "# \n",
    "1) Library imports\n",
    "2) Data imports\n",
    "3) Model architecture definition\n",
    "4) Training function definition\n",
    "5) Model training\n",
    "6) Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxCkcc2ArSiA"
   },
   "source": [
    "## Library imports \n",
    "(Place all library imports here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qdi1ymBrSiA",
    "outputId": "b3f9649f-15e3-45da-b14a-505714afdad9"
   },
   "outputs": [],
   "source": [
    "#KP - I just added the main ones from the labs.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import time # Tracking model training time.\n",
    "\n",
    "# Install mido for Data importing\n",
    "'''\n",
    "Pip install for COLAB:\n",
    "'''\n",
    "# !pip install mido;\n",
    "import mido\n",
    "from mido import MidiFile, Message, MidiTrack, MetaMessage\n",
    "import os\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3b7uuDcrqIj",
    "outputId": "eaf04f40-d5ff-40f2-b395-6854c3e36e8e"
   },
   "source": [
    "#Set working directory if required:\n",
    "''' For LOCAL:\n",
    "'''\n",
    "os.chdir('D:\\engsci\\year 3\\CLASS\\APS360\\project_pivot') #Sets current working directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "755DNUDOrSiB"
   },
   "source": [
    "## Data imports\n",
    "#### MIDI reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UGf_y0zxrSiB"
   },
   "outputs": [],
   "source": [
    "def CountTracks(directory):          #Count files and tracks in folder\n",
    "    trackCount = 0\n",
    "    fileCount = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".midi\"):\n",
    "            fileCount += 1\n",
    "            midiDir = MidiFile(directory+\"/\"+file)\n",
    "            for track in midiDir.tracks:\n",
    "                trackCount += 1\n",
    "    print(fileCount+\" files\")\n",
    "    print(trackCount+\" tracks\")\n",
    "\n",
    "    \n",
    "def PrintMessages(mid):                # print midi messages\n",
    "    for i, track in enumerate(mid.tracks):\n",
    "        print('Track {}: {}'.format(i, track.name))\n",
    "        for msg in track:\n",
    "            print(msg)\n",
    "\n",
    "            \n",
    "def PrintSomeMessages(mid):             #print first 200 midi messages\n",
    "    track = mid.tracks[1]\n",
    "    for i,msg in enumerate(track):\n",
    "        if i < 200:\n",
    "            print(msg)\n",
    "            \n",
    "def PrintMetaMessages(mid):             #print fmeta messages\n",
    "    track = mid.tracks[0]\n",
    "    for i,msg in enumerate(track):\n",
    "        print(msg)\n",
    "\n",
    "def cleanupMessages(mid):              #removes non-note messages by force\n",
    "    track = mid.tracks[1]\n",
    "    track2 = []\n",
    "    for msg in track:\n",
    "        if msg.type == \"note_on\":\n",
    "            track2.append(msg)\n",
    "    mid.tracks[1] = track2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UT23qIIrSiB"
   },
   "source": [
    "#### MIDI to Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zsvL0x4krSiB"
   },
   "outputs": [],
   "source": [
    "def Midi2NumpyNoSustain(mid, track0=1):                                #converts to numpy array removing non-note messages\n",
    "    track = mid.tracks[track0]                           #0th track only contains meta-messages, all notes on 1st track\n",
    "    notes = np.empty([0,4])\n",
    "    time = 0\n",
    "    for msg in track:\n",
    "        if msg.type == \"note_on\":                   # only count \"note\" messages - other inputs i.e. foot pedals are ignored\n",
    "            notes = np.append(notes,np.array([[msg.note, msg.velocity, msg.time + time, 0]]),axis=0)         # (note, velocity, time, sustain)\n",
    "            time = 0\n",
    "        elif msg.type == \"note_off\":\n",
    "            notes = np.append(notes,np.array([[msg.note, 0, msg.time + time, 0]]),axis=0)         # (note, velocity, time, sustain)\n",
    "            time = 0        \n",
    "        else:\n",
    "            time += msg.time                        #adjust time when removing other messages\n",
    "    return notes\n",
    "\n",
    "\n",
    "def NumpyGetSustain(note):\n",
    "    notes = np.copy(note)\n",
    "    for i, msg in enumerate(notes):\n",
    "        if msg[1] > 0:                            # if velocity is not 0\n",
    "            j = 1\n",
    "            sustain = 0\n",
    "            while msg[0] != notes[i+j][0]:        # while note values are different\n",
    "                sustain += notes[i+j][2]\n",
    "                j += 1                            #search for next message with same note i.e. message telling that note was released\n",
    "            notes[i,3] = sustain + notes[i+j][2]\n",
    "    time = 0\n",
    "    for i, msg in enumerate(notes):\n",
    "        if msg[1] > 0:\n",
    "            notes[i,2] += time\n",
    "            time = 0\n",
    "        else:\n",
    "            time += msg[2]                        #adjust time\n",
    "    notes = notes[notes[:,1] > 0]                 #filter for notes with positive velocities (note presses)\n",
    "    return notes\n",
    "\n",
    "def NumpyNormalize(note, oneHot=False, full=False):                         #normalize all values to 0-1\n",
    "    notes = np.copy(note)\n",
    "    \n",
    "    if oneHot:\n",
    "        if full:\n",
    "            notes[:,88] /= 128\n",
    "            notes[:,89] /= 40000\n",
    "            notes[:,90] /= 40000\n",
    "        else:\n",
    "            notes[:,12] /= 11\n",
    "            notes[:,13] /= 128\n",
    "            notes[:,14] /= 40000\n",
    "            notes[:,15] /= 40000\n",
    "    else:\n",
    "        notes[:,0] /= 128\n",
    "        notes[:,1] /= 128\n",
    "        notes[:,2] /= 40000\n",
    "        notes[:,3] /= 40000       \n",
    "    return notes\n",
    "\n",
    "def NumpyOneHot(note):\n",
    "    notes = np.copy(note)\n",
    "    oneHot = np.zeros([len(notes),16])\n",
    "    oneHot[:, 13:] = notes[:, 1:]\n",
    "    names = notes[:,0]\n",
    "    namesOct = names%12\n",
    "    oneHot[:,12] = (names-(namesOct))/12\n",
    "    \n",
    "    for i, name in enumerate(namesOct):\n",
    "        oneHot[i,name.astype(int)] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "def NumpyNotesOneHot(note):\n",
    "    notes = np.copy(note)\n",
    "    \n",
    "    oneHot = np.zeros([len(notes),91])\n",
    "    oneHot[:, 88:] = notes[:, 1:]\n",
    "    names = notes[:,0]-21\n",
    "    #namesOct = names%12\n",
    "    #oneHot[:,12] = (names-(namesOct))/12\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        oneHot[i,name.astype(int)] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "def Midi2Numpy(path, oneHot=False, track0=1): # full midi to numpy conversion\n",
    "    mid = MidiFile(path)\n",
    "    notes = Midi2NumpyNoSustain(mid, track0=track0)\n",
    "    cleanNotes = NumpyGetSustain(notes)\n",
    "    \n",
    "    if oneHot:\n",
    "        cleanNotes = NumpyOneHot(cleanNotes)\n",
    "    \n",
    "    normNotes = NumpyNormalize(cleanNotes, oneHot=oneHot)\n",
    "    return normNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ3C4NpgrSiB"
   },
   "source": [
    "#### Numpy to MIDI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FcyTTyH-rSiB"
   },
   "outputs": [],
   "source": [
    "def NumpyDenormalize(note): # interpret all values from 0-1 to normal values\n",
    "    notes = np.copy(note)    \n",
    "    if notes.shape[1] == 16: # if encode as one-hot\n",
    "        notes[:,12] *= 11 # octave\n",
    "        notes[:,13] *= 128 # vel\n",
    "        notes[:,14] *= 40000 # time\n",
    "        notes[:,15] *= 40000 # sustain\n",
    "        \n",
    "        notes = NumpyEncode(notes) #encode back as original 4-variable format\n",
    "        \n",
    "    elif notes.shape[1] == 91: # if encoded as one-hot w/o octave\n",
    "        notes[:,88] *= 128\n",
    "        notes[:,89] *= 40000\n",
    "        notes[:,90] *= 40000\n",
    "        \n",
    "        #print(notes)\n",
    "        \n",
    "        notes = NumpyEncodeNotes(notes)\n",
    "        \n",
    "    else:\n",
    "        notes[:,0] *= 128\n",
    "        notes[:,1] *= 128\n",
    "        notes[:,2] *= 40000\n",
    "        notes[:,3] *= 40000       \n",
    "    return notes.astype(int)\n",
    "\n",
    "def NumpyEncode(note): # convert back from one-hot encoding\n",
    "    notes = np.copy(note)    \n",
    "    encoded = np.zeros([len(notes),4]) # create array   \n",
    "    encoded[:, 1:] = notes[:, 13:] # set vel/time/sustain\n",
    "    encoded[:, 0] = notes[:,12]*12 # add octave value\n",
    "    \n",
    "    for i in range(len(notes)):\n",
    "        encoded[i,0] += np.argmax(notes[i,:12])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def NumpyEncodeNotes(note): # convert back from one-hot encoding\n",
    "    notes = np.copy(note)    \n",
    "    encoded = np.zeros([len(notes),4])\n",
    "    \n",
    "    encoded[:, 1:] = notes[:, 88:] # set vel/time/sustain\n",
    "    \n",
    "    for i in range(len(notes)):\n",
    "        encoded[i,0] += np.argmax(notes[i,:88])+21\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def NumpySequence(notes): # put all notes into a \"timeline\" i.e.: time values of [10, 20, 10, 30] become [10, 30, 40, 70]\n",
    "    sequenced = np.copy(notes)                      # this allows us to easily add vel=0 notes in any order since we can later sort them by time\n",
    "    for i, msg in enumerate(sequenced):\n",
    "        if i > 0:\n",
    "            sequenced[i,2] += sequenced[i-1,2]\n",
    "    return sequenced\n",
    "\n",
    "def NumpyAddOffNotes(sequenced): # add vel=0 notes from sustain into sequenced timeline\n",
    "    withOff = np.copy(sequenced)\n",
    "    for msg in sequenced:\n",
    "        offNote = np.array([[msg[0], 0, msg[2] + msg[3], 0]])\n",
    "        withOff = np.append(withOff, offNote, axis=0)\n",
    "    #withOff = np.sort(withOff,axis=0)\n",
    "    withOff = withOff[withOff[:,2].argsort()] # sort by time\n",
    "    return withOff\n",
    "\n",
    "def NumpyUnsequence(notes): # revert time value to \"time since last message\"\n",
    "    unsequenced = np.copy(notes)\n",
    "    for i, msg in reversed(list(enumerate(unsequenced))):\n",
    "        unsequenced[i,3] = 0\n",
    "        if i > 0:\n",
    "            unsequenced[i,2] -= unsequenced[i-1,2]\n",
    "    return unsequenced\n",
    "\n",
    "def Numpy2MidiDirect(array):    #make MIDI object from numpy\n",
    "    #Start with initializing a new Mido Track:\n",
    "    mid = MidiFile()\n",
    "    track0 = MidiTrack()\n",
    "    track1 = MidiTrack()\n",
    "    \n",
    "    track0.append(MetaMessage('set_tempo', tempo=500000, time=0)) #MetaMessages not necessary but are present in used files\n",
    "    track0.append(MetaMessage('time_signature', numerator=4, denominator=4, clocks_per_click=24, notated_32nd_notes_per_beat=8, time=0))\n",
    "    track0.append(MetaMessage('end_of_track', time=1))\n",
    "    \n",
    "    track1.append(Message('program_change', channel=0, program=0, time=0))\n",
    "    \n",
    "    for i,note in enumerate(array):         # Get the index and the note. Array must be int array\n",
    "        j = 1\n",
    "        track1.append(Message('note_on',note = array[i,0], velocity = array[i,1],time = array[i,2])) # Add the note to the track.\n",
    "\n",
    "    mid.tracks.append(track0)\n",
    "    mid.tracks.append(track1)\n",
    "    return mid\n",
    "\n",
    "def Numpy2Midi(notes, name): # full numpy to midi conversion, saving result to [name].midi\n",
    "    denorm = NumpyDenormalize(notes)\n",
    "    seq = NumpySequence(denorm)\n",
    "    off = NumpyAddOffNotes(seq)\n",
    "    unseq = NumpyUnsequence(off)\n",
    "    mid = Numpy2MidiDirect(unseq)\n",
    "    mid.save(name + \".midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OCUYyBiIrSiB"
   },
   "outputs": [],
   "source": [
    "def splitTrack(midiFileName, inputDir, outputDir):\n",
    "    mid = MidiFile(inputDir+midiFileName+\".midi\")\n",
    "    for t, track in enumerate(mid.tracks):\n",
    "        program, channel = findInstrument(track)        \n",
    "        if len(track) < 200 or program == -1:\n",
    "            continue\n",
    "        trackMidi = copy.copy(mid)\n",
    "        indices = [t]\n",
    "        trackMidi.tracks = [trackMidi.tracks[x] for x in indices]\n",
    "        #trackMidi.tracks.append(track)\n",
    "        trackMidi.save(outputDir + midiFileName + \"_prog{:0>3d}_chan{:0>2d}.midi\".format(program,channel))\n",
    "\n",
    "def splitAllTracks(inputDir, outputDir, first=0): # very bad code -  doesnt remove the .midi in the middle lol\n",
    "    for i,f in enumerate(os.listdir(inputDir)):\n",
    "        if i >= first:\n",
    "            if len(f) <7:\n",
    "                continue\n",
    "            splitTrack(f[:7],inputDir,outputDir)\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                print(f)\n",
    "        \n",
    "def findInstrument(track):\n",
    "    count = 0\n",
    "    infoMsg = 0\n",
    "    \n",
    "    for msg in track:\n",
    "        if msg.type == \"program_change\":\n",
    "            count += 1\n",
    "            infoMsg = msg\n",
    "    \n",
    "    if count >= 1:\n",
    "        if infoMsg.channel == 9 or count == 1:\n",
    "            return infoMsg.program, infoMsg.channel\n",
    "        else: \n",
    "            return -1, -1\n",
    "    else:\n",
    "        return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iP4lojMgrSiB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#splitAllTracks(\"data/jazz/\",\"data/jazz/tracks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTtFiHvFrSiB"
   },
   "source": [
    "#### Generatng tensor dataset from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vxt1UBhHrSiB"
   },
   "outputs": [],
   "source": [
    "def Numpy2Dataset(notes, tgt, num=100,skip=200): # make list of sumpy arrays\n",
    "    samples = []\n",
    "    targets = []\n",
    "    i = 0\n",
    "    while i+num <= len(notes):\n",
    "        samples.append(notes[i:i+num])\n",
    "        targets.append(tgt)\n",
    "        i += skip\n",
    "    return samples, targets\n",
    "\n",
    "def SampleAllNumpy(dataPath): # generate samples from all saved CSVs\n",
    "    allSamples = []\n",
    "    allTargets = []\n",
    "\n",
    "    for i,f in enumerate(os.listdir(dataPath)):\n",
    "        notes = np.genfromtxt(dataPath+f, delimiter=',')\n",
    "        tgt = GetGroup(f)\n",
    "                             \n",
    "        samples, targets = Numpy2Dataset(notes, tgt)\n",
    "        allSamples += samples\n",
    "        allTargets += targets\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    random.seed(0)\n",
    "    random.shuffle(allSamples)\n",
    "    random.seed(0)\n",
    "    random.shuffle(allTargets) # shuffle samples and targets in the exact same way\n",
    "    \n",
    "    return allSamples, allTargets\n",
    "\n",
    "def SaveSamplesTensor(samples, targets, outputPath, name=\"Notes_Dataset\"): # save tensor\n",
    "    tens = torch.Tensor(samples)\n",
    "    targ = torch.Tensor(targets)\n",
    "    dataset = TensorDataset(tens,targ)\n",
    "    torch.save(dataset, outputPath+name+\".pt\")\n",
    "    return tens   \n",
    "\n",
    "def SaveAllSamples(dataPath, outputPath, name=\"Notes_Dataset\"): # save dataset tensor\n",
    "    samples, targets = SampleAllNumpy(dataPath)\n",
    "    SaveSamplesTensor(samples, targets, outputPath, name)\n",
    "    \n",
    "def SplitSamples(dataset, ranges):\n",
    "    torch.maual_seed(0)\n",
    "    trainData, valData, testData = random_split(oneHotDataset, splitRange)\n",
    "    return trainData, valData, testData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DGM_cpI2rSiB"
   },
   "outputs": [],
   "source": [
    "def ProgramGroup(program, channel):\n",
    "    if channel == 9:\n",
    "        return 0 # drums\n",
    "    if program < 8:\n",
    "        return 1 # piano\n",
    "    if program < 16:\n",
    "        return 8 # pitched percussion\n",
    "    if program < 24:\n",
    "        return 7 # organ\n",
    "    if program < 32:\n",
    "        return 3 # guitar\n",
    "    if program < 40:\n",
    "        return 2 # bass\n",
    "    if program < 48:\n",
    "        return 9 # string\n",
    "    if program < 56:\n",
    "        return 10 # ensemble\n",
    "    if program < 64:\n",
    "        return 4 # brass \n",
    "    if program < 72:\n",
    "        return 5 # reed\n",
    "    if program < 80:\n",
    "        return 11 # pipe\n",
    "    if program < 88:\n",
    "        return 6 # synth lead\n",
    "    if program < 96:\n",
    "        return 12 # synth pad\n",
    "    if program < 104:\n",
    "        return 13 # synth efects\n",
    "    if program < 112:\n",
    "        return 14 # ethnic\n",
    "    if program < 120:\n",
    "        return 15 # percusson\n",
    "    else:\n",
    "        return 16 # other \n",
    "    \n",
    "def GetProgram(name):\n",
    "    prog = name[12:15]\n",
    "    chan = name[20:22]\n",
    "    return int(prog), int(chan)\n",
    "\n",
    "def GetGroup(name):\n",
    "    prog, chan = GetProgram(name)\n",
    "    group = ProgramGroup(prog, chan)\n",
    "    return(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcsmXvAarSiB"
   },
   "source": [
    "#### Bulk data conversion code - COMMENT OUT IF NOT IN USE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vKGMTqwurSiB"
   },
   "outputs": [],
   "source": [
    "#SaveAllSamples(\"data/jazz/tracks_numpy/train/\",\"data/\",\"trainDataClass\") #save all into tensor\n",
    "#SaveAllSamples(\"data/jazz/tracks_numpy/val/\",\"data/\",\"valDataClass\") #save all into tensor\n",
    "#SaveAllSamples(\"data/jazz/tracks_numpy/test/\",\"data/\",\"testDataClass\") #save all into tensor\n",
    "\n",
    "# oneHotDataset = torch.load(\"data/onehot_data/onehot.pt\")\n",
    "\n",
    "# splitProp = np.array([0.9, 0.05 , 0.05]) # 90/5/5% split\n",
    "# splitRange = len(oneHotDataset)*splitProp\n",
    "# splitRange[0] += 1                           # so numbers add up\n",
    "# splitRange = splitRange.astype(int)\n",
    "\n",
    "# length = len(oneHotDataset)\n",
    "# a = 0.9*length\n",
    "# a = int(a)\n",
    "\n",
    "# b = 0.95*length\n",
    "# b = int(b)\n",
    "\n",
    "# tiny = oneHotDataset[:30]\n",
    "# train = oneHotDataset[:a]\n",
    "# val = oneHotDataset[a:b]\n",
    "# test = oneHotDataset[b:]\n",
    "\n",
    "# tinyData = TensorDataset(tiny[0].clone(), tiny[1].clone())\n",
    "# trainData = TensorDataset(train[0].clone(),train[1].clone())\n",
    "# valData = TensorDataset(val[0].clone(),val[1].clone())\n",
    "# testData = TensorDataset(test[0].clone(),test[1].clone())\n",
    "\n",
    "# torch.save(tinyData,\"data/onehot_data/tinyDataOneHot.pt\")\n",
    "# torch.save(trainData,\"data/onehot_data/trainDataOneHot.pt\")\n",
    "# torch.save(valData,\"data/onehot_data/valDataOneHot.pt\")\n",
    "# torch.save(testData,\"data/onehot_data/testDataOneHot.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ssp85azWrSiC"
   },
   "outputs": [],
   "source": [
    "#a = torch.load(\"data/va.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "63avpbMHrSiC",
    "outputId": "38903c44-8edc-4f0b-9005-213cafdb812e"
   },
   "outputs": [],
   "source": [
    "# for img, label in iter(a):\n",
    "#     #print(img)\n",
    "#     print(label)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dHJs765krSiC"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: COMMENT OUT IF NOT IN USE TO AVOID ACCIDENTS!!!!!!!\n",
    "\n",
    "# Getting CSVs from MIDI data and processed data from CSVs\n",
    "# Processed MIDI does not contain program messages and so are a good measure of what output SHOULD look like in a perfect world\n",
    "\n",
    "# dataPath = \"data/jazz/tracks/\"\n",
    "# outputPath = \"data/jazz/tracks_numpy/\"\n",
    "# #processedPath = \"data/MIDI_files_processed/\"\n",
    "\n",
    "# for i,f in enumerate(os.listdir(dataPath)):\n",
    "#     if i > 2700:\n",
    "#         notes = Midi2Numpy(dataPath+f,track0=0)\n",
    "#         np.savetxt(outputPath + f[:22]+\".csv\",notes,delimiter=\",\")\n",
    "#     #Numpy2Midi(notes, processedPath + \"MIDI_{:04d}\".format(i))\n",
    "#     #break\n",
    "    \n",
    "#     if i % 200 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GmrcKT1-rSiC"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: COMMENT OUT IF NOT IN USE TO AVOID ACCIDENTS!!!!!!!\n",
    "\n",
    "# Getting CSVs from MIDI data and processed data from CSVs\n",
    "# Processed MIDI does not contain program messages and so are a good measure of what output SHOULD look like in a perfect world\n",
    "\n",
    "# dataPath = \"data/jazz/tracks/\"\n",
    "# outputPath = \"data/jazz/tracks_numpy/\"\n",
    "# #processedPath = \"data/MIDI_files_processed/\"\n",
    "\n",
    "# for i,f in enumerate(os.listdir(dataPath)):\n",
    "#     if i == 47:\n",
    "#         print(f)\n",
    "#         testMidi = MidiFile(dataPath+f)\n",
    "#         #PrintMessages(testMidi)\n",
    "#         print(GetGroup(f))\n",
    "#     #Numpy2Midi(notes, processedPath + \"MIDI_{:04d}\".format(i))\n",
    "#     #break\n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r4lW9eKUrSiC"
   },
   "outputs": [],
   "source": [
    "# dataPath = \"data/numpy_files/\"  # one-hot encoding on CSVs\n",
    "# outputPath = \"data/numpy_full/\"\n",
    "\n",
    "# for i,f in enumerate(os.listdir(dataPath)):\n",
    "#     notes = np.genfromtxt(dataPath+f, delimiter=',')\n",
    "#     notes = NumpyDenormalize(notes)\n",
    "#     notes = NumpyNotesOneHot(notes)\n",
    "#     notes = NumpyNormalize(notes, oneHot=True, full=True)\n",
    "#     np.savetxt(outputPath + \"MIDI_{:04d}.csv\".format(i),notes,delimiter=\",\")\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0fsM7MtrSiC"
   },
   "source": [
    "## Baseline Model Code\n",
    "#### getting available notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cqmkQTqrSiC"
   },
   "source": [
    "## Model architecture definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiA0KsAkrSiC"
   },
   "source": [
    "Set the hyperparameters below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4gpkIALrSiC",
    "outputId": "48c295e0-fcac-4b7a-aa4f-536bf2bef54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class created succesfully\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden, n_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.name = \"LSTMClassifier\"\n",
    "        self.n_features, self.n_classes, self.n_hidden, self.n_layers = n_features, n_classes, n_hidden, n_layers, \n",
    "        self.lstm = nn.LSTM(n_features, n_hidden, n_layers, batch_first=True)\n",
    "        self.fc0 = torch.nn.Linear(2*n_hidden, n_hidden)\n",
    "        self.fc1 = torch.nn.Linear(n_hidden, int(n_hidden/2))\n",
    "        self.fc2 = torch.nn.Linear(int(n_hidden/2), n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagate the RNN\n",
    "        h0 = torch.zeros(self.n_layers,x.size(0),self.n_hidden)\n",
    "        c0 = torch.zeros(self.n_layers,x.size(0),self.n_hidden)\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            c0 = c0.cuda()\n",
    "            h0 = h0.cuda()\n",
    "        out, _ = self.lstm(x)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = torch.cat([torch.max(out, dim=1)[0], \n",
    "                    torch.mean(out, dim=1)], dim=1) #Combine max and mean\n",
    "        \n",
    "        out = self.fc0(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        output = self.fc2(out)\n",
    "        return output\n",
    "\n",
    "print('Model class created succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4gpkIALrSiC",
    "outputId": "48c295e0-fcac-4b7a-aa4f-536bf2bef54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model class created succesfully\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "class BaselineClassifier(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden, n_layers):\n",
    "        super(BaselineClassifier, self).__init__()\n",
    "        self.name = \"BaseClassifier\"\n",
    "        self.fc1 = torch.nn.Linear(131*50, n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat = x.view(-1,131*50)\n",
    "        out = self.fc1(flat)\n",
    "        out = F.relu(out)\n",
    "        output = self.fc2(out)\n",
    "        return output\n",
    "\n",
    "print('Baseline Model class created succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D2idmcbrSiD"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "G5ENRYH9rSiD"
   },
   "outputs": [],
   "source": [
    "#To help us save the model easier...\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GLOGDWmrubIm"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader): #Accuracy on note selection...\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for sample in data_loader:\n",
    "        inputs, labels = sample\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "        output = model(inputs)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        #if i%100 == 0:\n",
    "            #print(pred)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += len(labels)\n",
    "        i += 1\n",
    "    return (correct / total)\n",
    "\n",
    "def get_loss(model, data_loader, criterion):\n",
    "    total_loss = 0\n",
    "    for sample in data_loader:\n",
    "        inputs, labels = sample\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "        output = model(inputs)\n",
    "        # Get loss\n",
    "        loss = criterion(output, labels.long())\n",
    "#         loss.backward() P SURE THIS SHOULD NOT BE HERE\n",
    "\n",
    "        # Sum up loss\n",
    "        total_loss += float(loss)\n",
    "        \n",
    "    lossVal = total_loss / len(inputs)\n",
    "    return lossVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Fpju80lNTuoE"
   },
   "outputs": [],
   "source": [
    "# Training Curve\n",
    "def plot_training_curve(accLoss, train, val):\n",
    "    \"\"\" Plots the training curve for a model's train/validation loss or accuracy.\n",
    "\n",
    "    Args:\n",
    "        accLoss: \"Accuracy\" or \"Loss\"\n",
    "        train: train data\n",
    "        val: validation data\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    titleStr = \"Train vs Validation \" + accLoss\n",
    "    plt.title(titleStr)\n",
    "    n = len(train) # number of epochs\n",
    "    plt.plot(range(1,n+1), train, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(accLoss)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8KDJooOSVCTi"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset, num_epochs=5, batch_size=64, learning_rate=1e-3,model_name = 'model'):\n",
    "    torch.manual_seed(1000) #Fixed. Make sure we use this throughout...\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, weight_decay=0.001)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    startTime=time.time() #Start of training\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        print(\"Using Cuda!\")\n",
    "    \n",
    "    maxVal = 0\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "#             print('labels: ',labels)\n",
    "            out = model(inputs)             # forward pass\n",
    "#             print(out.shape)\n",
    "            loss = criterion(out, labels.long()) # compute the total loss\n",
    "\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()              # make the updates for each parameter\n",
    "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "\n",
    "            loss = float(loss)/batch_size             # compute *average* loss\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader)\n",
    "        val_loss = get_loss(model, val_loader, criterion)\n",
    "        val_acc =get_accuracy(model, val_loader)\n",
    "\n",
    "#         print('Epoch: {} - Train loss: {:.4f}, Validation loss: {:.4f}, Train accuracy: {:.2f}%, Validation accuracy: {:.2f}%'.format(\n",
    "#                             epoch+1, float(loss), float(val_loss), float(train_acc), float(val_acc)))\n",
    "        \n",
    "        train_losses.append(loss)             # compute *average* loss\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if val_acc > maxVal and val_acc > 0.3:\n",
    "            maxVal = val_acc\n",
    "            torch.save(model.state_dict(), model_name+\".pt\")\n",
    "            print()\n",
    "            print(\"Validation record in epoch {}: {}\".format(epoch, maxVal)) # save model when validation accuracy is higher than previousruns\n",
    "            print()\n",
    "#         if (train_acc > val_acc + 0.5) and maxVal > 0.8: #check for overfitting\n",
    "#             break\n",
    "            \n",
    "            \n",
    "        #n += 1\n",
    "#         if epoch % 10 == 0:\n",
    "#             perc = epoch/num_epochs\n",
    "#             print(\"{:.2%} complete\".format(perc))\n",
    "\n",
    "        if epoch % 10 == 0 or epoch==1:\n",
    "            perc = (epoch+1)/num_epochs\n",
    "            currTime = time.time() - startTime\n",
    "            totalTime = currTime/perc\n",
    "            remaining = totalTime - currTime\n",
    "\n",
    "            remHour = int(remaining/3600)\n",
    "            remaining -= remHour*3600\n",
    "\n",
    "            remMin = int(remaining/60)\n",
    "            remaining -= remMin*60\n",
    "            print(\"{:.2%} complete, {}:{}:{} to go!\".format(perc, remHour, remMin, int(remaining)))\n",
    "\n",
    "\n",
    "    #Save the file current model with the weights\n",
    "    model_path = get_model_name(model.name, batch_size, learning_rate, num_epochs)\n",
    "    #torch.save(model.state_dict(), model_path)\n",
    "    end_time= time.time()\n",
    "    #torch.save(model.state_dict(), model_name+\".pt\")\n",
    "    return train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DyDP2xejwnXc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWnS:\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved tensor dataset\n",
    "'''\n",
    "For colab:\n",
    "'''\n",
    "# train_dataset = torch.load(r'/content/gdrive/My Drive/APS360/Project/trainDataClass.pt')\n",
    "# val_dataset = torch.load(r'/content/gdrive/My Drive/APS360/Project/valDataClass.pt')\n",
    "# test_dataset = torch.load(r'/content/gdrive/My Drive/APS360/Project/testDataClass.pt')\n",
    "'''\n",
    "For local: (@Kevin)\n",
    "'''\n",
    "'''\n",
    "onehot:\n",
    "'''\n",
    "#train_dataset = torch.load(\"data/ClassData/onehot/trainDataClassOnehot.pt\")\n",
    "#val_dataset = torch.load(\"data/ClassData/onehot/valDataClassOnehot.pt\")\n",
    "#test_dataset = torch.load(\"data/ClassData/onehot/testDataClassOnehot.pt\")\n",
    "\n",
    "'''\n",
    "harmony:\n",
    "'''\n",
    "#train_dataset = torch.load(\"data/alt/similar/trainDataClassOnehot_harmony.pt\")\n",
    "#val_dataset = torch.load(\"data/alt/similar/valDataClassOnehot_harmony.pt\")\n",
    "#test_dataset = torch.load(\"data/alt/harmony/trainDataClassOnehot_harmony\")\n",
    "\n",
    "'''\n",
    "similar:\n",
    "'''\n",
    "train_dataset = torch.load(\"data/alt/similar/trainDataClassOnehot_similar.pt\")\n",
    "val_dataset = torch.load(\"data/alt/similar/valDataClassOnehot_similar.pt\")\n",
    "test_dataset = torch.load(\"data/alt/similar/trainDataClassOnehot_similar.pt\")\n",
    "\n",
    "'''\n",
    "WnS:\n",
    "'''\n",
    "#train_dataset = torch.load(\"data/alt/WnS/trainDataClassOnehot_WnS.pt\")\n",
    "#val_dataset = torch.load(\"data/alt/WnS/valDataClassOnehot_WnS.pt\")\n",
    "#test_dataset = torch.load(\"data/alt/harmony/trainDataClassOnehot_harmony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print stats from the dataset\n",
    "for imgs, labels in torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True):\n",
    "#     print(labels)\n",
    "    labels = labels\n",
    "\n",
    "hist_var = np.zeros((7))\n",
    "for i in range(len(labels)):\n",
    "#     print(labels[i].numpy())\n",
    "    #Count each class samples...\n",
    "    for j in range(0,7):\n",
    "        if labels[i] == j:\n",
    "            hist_var[j] += 1\n",
    "print(hist_var) #Good visual for the demo! (see plot below :) )\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(0,6,7),hist_var,\"o\")\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Class')\n",
    "plt.title('TrainData Class Distributions')\n",
    "plt.show()\n",
    "\n",
    "for imgs, labels in torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True):\n",
    "#     print(labels)\n",
    "    labels = labels\n",
    "\n",
    "hist_var = np.zeros((7))\n",
    "for i in range(len(labels)):\n",
    "#     print(labels[i].numpy())\n",
    "    #Count each class samples...\n",
    "    for j in range(0,7):\n",
    "        if labels[i] == j:\n",
    "            hist_var[j] += 1\n",
    "print(hist_var) #Good visual for the demo! (see plot below :) )\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(0,6,7),hist_var,\"o\")\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Class')\n",
    "plt.title('valData Class Distributions')\n",
    "plt.show()\n",
    "\n",
    "for imgs, labels in torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True):\n",
    "#     print(labels)\n",
    "    labels = labels\n",
    "\n",
    "hist_var = np.zeros((7))\n",
    "for i in range(len(labels)):\n",
    "#     print(labels[i].numpy())\n",
    "    #Count each class samples...\n",
    "    for j in range(0,7):\n",
    "        if labels[i] == j:\n",
    "            hist_var[j] += 1\n",
    "print(hist_var) #Good visual for the demo! (see plot below :) )\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(0,6,7),hist_var,\"o\")\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Class')\n",
    "plt.title('testData Class Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WEJ4OXb67IUX"
   },
   "outputs": [],
   "source": [
    "# # train_dataset = get_class_3(train_dataset,0,1,2)\n",
    "# # val_dataset = get_class_3(val_dataset,0,1,2)\n",
    "# # test_dataset = get_class_3(test_dataset,0,1,2)\n",
    "\n",
    "# #Run the 2,5,6\n",
    "# train_dataset = get_class_7(train_dataset,0,1,2,3,4,5,6)\n",
    "# val_dataset = get_class_7(val_dataset,0,1,2,3,4,5,6)\n",
    "# test_dataset = get_class_7(test_dataset,0,1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Hr_c4ReHx191"
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# For LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html #torch.nn.LSTM \n",
    "# Hyperparameters\n",
    "#Run the 2 and the 6 and the 5\n",
    "N_FEATURES = 131\n",
    "N_CLASSES = 4\n",
    "N_HIDDEN = 16\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Hr_c4ReHx191"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = LSTMClassifier(n_features=N_FEATURES, n_classes = N_CLASSES, n_hidden=N_HIDDEN, n_layers=N_LAYERS)\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr_c4ReHx191"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "baseline = BaselineClassifier(n_features=N_FEATURES, n_classes = N_CLASSES, n_hidden=128, n_layers=N_LAYERS)\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    baseline.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr_c4ReHx191"
   },
   "outputs": [],
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train(baseline, \n",
    "    train_dataset, val_dataset, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,model_name='baseline2')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0RWdIsr-oLi",
    "outputId": "f98f71c5-86c2-44b9-a35f-326e6da2b249"
   },
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train(model, \n",
    "    train_dataset, val_dataset, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,model_name='Sergio_final2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Ue6n6_07WJFW",
    "outputId": "2489c0d7-1818-496f-889f-9cc0f885a203"
   },
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_training_curve(\"Loss\", train_losses, val_losses)\n",
    "\n",
    "# Plot accuracy curves\n",
    "plot_training_curve(\"Accuracy\", train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22023\n",
      "2033\n",
      "torch.Size([2033, 1])\n",
      "confusion matrix from validation set\n",
      "[[0.94129159 0.02935421 0.01369863 0.01565558]\n",
      " [0.00673401 0.97306397 0.003367   0.01683502]\n",
      " [0.0044843  0.01121076 0.79147982 0.19282511]\n",
      " [0.         0.05186722 0.13278008 0.8153527 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "model = model.cpu()\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     inputs, y_true = data\n",
    "\n",
    "# #             print('labels: ',labels)\n",
    "#     output = model(inputs)             # forward pass\n",
    "#     y_pred = output.max(1, keepdim=True)[1]\n",
    "# print(y_pred.shape)\n",
    "# y_pred = y_pred.detach().numpy()\n",
    "# y_true = y_true.detach().numpy()\n",
    "# # print(y_pred)\n",
    "# print(\"confusion matrix from trainin set\") #https://codeyarns.com/tech/2014-10-24-how-to-create-a-confusion-matrix-plot-using-matplotlib.html\n",
    "# m = confusion_matrix(y_true,y_pred,normalize = 'true')\n",
    "# plt.matshow(m)\n",
    "# plt.colorbar()\n",
    "\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_true = data\n",
    "\n",
    "#             print('labels: ',labels)\n",
    "    output = model(inputs)             # forward pass\n",
    "    y_pred = output.max(1, keepdim=True)[1]\n",
    "print(y_pred.shape)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_true = y_true.detach().numpy()\n",
    "# print(y_pred)\n",
    "print(\"confusion matrix from validation set\") #https://codeyarns.com/tech/2014-10-24-how-to-create-a-confusion-matrix-plot-using-matplotlib.html\n",
    "m = confusion_matrix(y_true,y_pred,normalize = 'true')\n",
    "plt.matshow(m)\n",
    "plt.colorbar()\n",
    "#plt.xlabel()\n",
    "print(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load(\"Sergio_similar1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = dataList[13]\n",
    "print(b)\n",
    "a = a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(a).max(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtmDqDHmrSiD"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOIt01mprSiD",
    "outputId": "d2e266f1-d4b8-42d3-b679-d90c77bda27a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.1411\n",
      "Test Accuracy: 90.46%\n",
      "torch.Size([22023, 1])\n",
      "confusion matrix from validation set\n",
      "[[99  1  1  0]\n",
      " [ 1 96  2  1]\n",
      " [ 1  2 79 18]\n",
      " [ 0  2 10 88]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Only run at the end...\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = get_accuracy(model, test_loader)\n",
    "test_loss = get_loss(model, test_loader, criterion)\n",
    "model = model.cpu()\n",
    "print((\"Test Loss: {:.4f}\").format(float(test_loss)))\n",
    "print((\"Test Accuracy: {:.2%}\").format(float(test_acc)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "for i, data in enumerate(test_loader):\n",
    "    inputs, y_true = data\n",
    "\n",
    "#             print('labels: ',labels)\n",
    "    output = model(inputs)             # forward pass\n",
    "    y_pred = output.max(1, keepdim=True)[1]\n",
    "print(y_pred.shape)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_true = y_true.detach().numpy()\n",
    "# print(y_pred)\n",
    "print(\"confusion matrix from validation set\") #https://codeyarns.com/tech/2014-10-24-how-to-create-a-confusion-matrix-plot-using-matplotlib.html\n",
    "m = confusion_matrix(y_true,y_pred,normalize = 'true')\n",
    "plt.matshow(m)\n",
    "plt.colorbar()\n",
    "\n",
    "m = m*100\n",
    "m = m.round()\n",
    "m=m.astype(int)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(n_features=N_FEATURES, n_classes = N_CLASSES, n_hidden=N_HIDDEN, n_layers=N_LAYERS)\n",
    "\n",
    "model.load_state_dict(torch.load('D:/engsci/year 3/CLASS/APS360/project_pivot/7_class_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Project_workbook_inst_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
