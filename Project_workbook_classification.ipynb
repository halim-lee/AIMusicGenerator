{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Project_workbook_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzXloeN6jjLd"
      },
      "source": [
        "# Project File - APS360 Team 25\n",
        "Divided into the following section: \n",
        "# \n",
        "1) Library imports\n",
        "2) Data imports\n",
        "3) Model architecture definition\n",
        "4) Training function definition\n",
        "5) Model training\n",
        "6) Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XAQ-54EjjLe"
      },
      "source": [
        "## Library imports \n",
        "(Place all library imports here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTJQZVdDjjLf"
      },
      "source": [
        "#import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time # Tracking model training time."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T67zGCXWnDV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a709887-bb18-4456-a3d5-dfa1c4a50af9"
      },
      "source": [
        "# Install mido for Data importing\n",
        "!pip install mido;\n",
        "\n",
        "import mido\n",
        "from mido import MidiFile, Message, MidiTrack, MetaMessage\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mido in /usr/local/lib/python3.6/dist-packages (1.2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iv0dOlJpBsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ba66c9-7873-4dcd-aabc-0d087ab7979e"
      },
      "source": [
        "#mount googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# location on Google Drive\n",
        "master_path = '/content/gdrive/My Drive/APS360/Project/'\n",
        "\n",
        "#Set working directory if required:\n",
        "%cd /content/gdrive/My\\ Drive/APS360/Project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/APS360/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGUNjsjljjLl"
      },
      "source": [
        "## Data imports\n",
        "#### MIDI reading functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVVI_Cc_jjLm"
      },
      "source": [
        "def CountTracks(directory):          #Count files and tracks in folder\n",
        "    trackCount = 0\n",
        "    fileCount = 0\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith(\".midi\"):\n",
        "            fileCount += 1\n",
        "            midiDir = MidiFile(directory+\"/\"+file)\n",
        "            for track in midiDir.tracks:\n",
        "                trackCount += 1\n",
        "    print(fileCount+\" files\")\n",
        "    print(trackCount+\" tracks\")\n",
        "\n",
        "    \n",
        "def PrintMessages(mid):                # print midi messages\n",
        "    for i, track in enumerate(mid.tracks):\n",
        "        print('Track {}: {}'.format(i, track.name))\n",
        "        for msg in track:\n",
        "            print(msg)\n",
        "\n",
        "            \n",
        "def PrintSomeMessages(mid):             #print first 200 midi messages\n",
        "    track = mid.tracks[1]\n",
        "    for i,msg in enumerate(track):\n",
        "        if i < 200:\n",
        "            print(msg)\n",
        "            \n",
        "def PrintMetaMessages(mid):             #print fmeta messages\n",
        "    track = mid.tracks[0]\n",
        "    for i,msg in enumerate(track):\n",
        "        print(msg)\n",
        "\n",
        "def cleanupMessages(mid):              #removes non-note messages by force\n",
        "    track = mid.tracks[1]\n",
        "    track2 = []\n",
        "    for msg in track:\n",
        "        if msg.type == \"note_on\":\n",
        "            track2.append(msg)\n",
        "    mid.tracks[1] = track2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlxYyJlvjjLp"
      },
      "source": [
        "#### MIDI to Numpy code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PwuX5ACjjLp"
      },
      "source": [
        "def Midi2NumpyNoSustain(mid):                                #converts to numpy array removing non-note messages\n",
        "    track = mid.tracks[1]                           #0th track only contains meta-messages, all notes on 1st track\n",
        "    notes = np.empty([0,4])\n",
        "    time = 0\n",
        "    for msg in track:\n",
        "        if msg.type == \"note_on\":                   # only count \"note\" messages - other inputs i.e. foot pedals are ignored\n",
        "            notes = np.append(notes,np.array([[msg.note, msg.velocity, msg.time + time, 0]]),axis=0)         # (note, velocity, time, sustain)\n",
        "            time = 0\n",
        "        else:\n",
        "            time += msg.time                        #adjust time when removing other messages\n",
        "    return notes\n",
        "\n",
        "\n",
        "def NumpyGetSustain(note):\n",
        "    notes = np.copy(note)\n",
        "    for i, msg in enumerate(notes):\n",
        "        if msg[1] > 0:                            # if velocity is not 0\n",
        "            j = 1\n",
        "            sustain = 0\n",
        "            while msg[0] != notes[i+j][0]:        # while note values are different\n",
        "                sustain += notes[i+j][2]\n",
        "                j += 1                            #search for next message with same note i.e. message telling that note was released\n",
        "            notes[i,3] = sustain + notes[i+j][2]\n",
        "    time = 0\n",
        "    for i, msg in enumerate(notes):\n",
        "        if msg[1] > 0:\n",
        "            notes[i,2] += time\n",
        "            time = 0\n",
        "        else:\n",
        "            time += msg[2]                        #adjust time\n",
        "    notes = notes[notes[:,1] > 0]                 #filter for notes with positive velocities (note presses)\n",
        "    return notes\n",
        "\n",
        "def NumpyNormalize(note, oneHot=False):                         #normalize all values to 0-1\n",
        "    notes = np.copy(note)\n",
        "    \n",
        "    if oneHot:\n",
        "        notes[:,12] /= 11\n",
        "        notes[:,13] /= 128\n",
        "        notes[:,14] /= 40000\n",
        "        notes[:,15] /= 40000\n",
        "    else:\n",
        "        notes[:,0] /= 128\n",
        "        notes[:,1] /= 128\n",
        "        notes[:,2] /= 40000\n",
        "        notes[:,3] /= 40000       \n",
        "    return notes\n",
        "\n",
        "def NumpyOneHot(note):\n",
        "    notes = np.copy(note)\n",
        "    oneHot = np.zeros([len(notes),16])\n",
        "    oneHot[:, 13:] = notes[:, 1:]\n",
        "    names = notes[:,0]\n",
        "    namesOct = names%12\n",
        "    oneHot[:,12] = (names-(namesOct))/12\n",
        "    \n",
        "    for i, name in enumerate(namesOct):\n",
        "        oneHot[i,name.astype(int)] = 1\n",
        "    \n",
        "    return oneHot\n",
        "\n",
        "def Midi2Numpy(path, oneHot=False): # full midi to numpy conversion\n",
        "    mid = MidiFile(path)\n",
        "    notes = Midi2NumpyNoSustain(mid)\n",
        "    cleanNotes = NumpyGetSustain(notes)\n",
        "    \n",
        "    if oneHot:\n",
        "        cleanNotes = NumpyOneHot(cleanNotes)\n",
        "    \n",
        "    normNotes = NumpyNormalize(cleanNotes, oneHot=oneHot)\n",
        "    return normNotes"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwph1iNajjLs"
      },
      "source": [
        "#### Numpy to MIDI code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aG5_vp-jjLt"
      },
      "source": [
        "def NumpyDenormalize(note): # interpret all values from 0-1 to normal values\n",
        "    notes = np.copy(note)    \n",
        "    if notes.shape[1] == 16: # if encode as one-hot\n",
        "        notes[:,12] *= 11\n",
        "        notes[:,13] *= 128\n",
        "        notes[:,14] *= 40000\n",
        "        notes[:,15] *= 40000\n",
        "        \n",
        "        notes = NumpyEncode(notes) #encode back as original 4-variable format\n",
        "    else:\n",
        "        notes[:,0] *= 128\n",
        "        notes[:,1] *= 128\n",
        "        notes[:,2] *= 40000\n",
        "        notes[:,3] *= 40000       \n",
        "    return notes.astype(int)\n",
        "\n",
        "def NumpyEncode(note): # convert back from one-hot encoding\n",
        "    notes = np.copy(note)\n",
        "    encoded = np.zeros([len(notes),4])\n",
        "    encoded[:, 1:] = notes[:, 13:]\n",
        "    encoded[:, 0] = notes[:,12]*12\n",
        "    \n",
        "    for i in range(len(notes)):\n",
        "        encoded[i,0] += np.argmax(notes[i,:12])\n",
        "    \n",
        "    return encoded\n",
        "\n",
        "def NumpySequence(notes): # put all notes into a \"timeline\" i.e.: time values of [10, 20, 10, 30] become [10, 30, 40, 70]\n",
        "    sequenced = np.copy(notes)                      # this allows us to easily add vel=0 notes in any order since we can later sort them by time\n",
        "    for i, msg in enumerate(sequenced):\n",
        "        if i > 0:\n",
        "            sequenced[i,2] += sequenced[i-1,2]\n",
        "    return sequenced\n",
        "\n",
        "def NumpyAddOffNotes(sequenced): # add vel=0 notes from sustain into sequenced timeline\n",
        "    withOff = np.copy(sequenced)\n",
        "    for msg in sequenced:\n",
        "        offNote = np.array([[msg[0], 0, msg[2] + msg[3], 0]])\n",
        "        withOff = np.append(withOff, offNote, axis=0)\n",
        "    #withOff = np.sort(withOff,axis=0)\n",
        "    withOff = withOff[withOff[:,2].argsort()] # sort by time\n",
        "    return withOff\n",
        "\n",
        "def NumpyUnsequence(notes): # revert time value to \"time since last message\"\n",
        "    unsequenced = np.copy(notes)\n",
        "    for i, msg in reversed(list(enumerate(unsequenced))):\n",
        "        unsequenced[i,3] = 0\n",
        "        if i > 0:\n",
        "            unsequenced[i,2] -= unsequenced[i-1,2]\n",
        "    return unsequenced\n",
        "\n",
        "def Numpy2MidiDirect(array):    #make MIDI object from numpy\n",
        "    #Start with initializing a new Mido Track:\n",
        "    mid = MidiFile()\n",
        "    track0 = MidiTrack()\n",
        "    track1 = MidiTrack()\n",
        "    \n",
        "    track0.append(MetaMessage('set_tempo', tempo=500000, time=0)) #MetaMessages not necessary but are present in used files\n",
        "    track0.append(MetaMessage('time_signature', numerator=4, denominator=4, clocks_per_click=24, notated_32nd_notes_per_beat=8, time=0))\n",
        "    track0.append(MetaMessage('end_of_track', time=1))\n",
        "    \n",
        "    track1.append(Message('program_change', channel=0, program=0, time=0))\n",
        "    \n",
        "    for i,note in enumerate(array):         # Get the index and the note. Array must be int array\n",
        "        j = 1\n",
        "        track1.append(Message('note_on',note = array[i,0], velocity = array[i,1],time = array[i,2])) # Add the note to the track.\n",
        "\n",
        "    mid.tracks.append(track0)\n",
        "    mid.tracks.append(track1)\n",
        "    return mid\n",
        "\n",
        "def Numpy2Midi(notes, name): # full numpy to midi conversion, saving result to [name].midi\n",
        "    denorm = NumpyDenormalize(notes)\n",
        "    seq = NumpySequence(denorm)\n",
        "    off = NumpyAddOffNotes(seq)\n",
        "    unseq = NumpyUnsequence(off)\n",
        "    mid = Numpy2MidiDirect(unseq)\n",
        "    mid.save(name + \".midi\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZmupxvijjLv"
      },
      "source": [
        "#### Generatng tensor dataset from CSVs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGT7Y7KqjjLw"
      },
      "source": [
        "def Numpy2Dataset(notes,num=20,skip=10): # make list of sumpy arrays\n",
        "    samples = []\n",
        "    i = 0\n",
        "    while i+num <= len(notes):\n",
        "        samples.append(notes[i:i+num])\n",
        "        i += skip\n",
        "    return samples\n",
        "\n",
        "def SampleAllNumpy(dataPath): # generate samples from all saved CSVs\n",
        "    allSamples = []\n",
        "\n",
        "    for i,f in enumerate(os.listdir(dataPath)):\n",
        "        notes = np.genfromtxt(dataPath+f, delimiter=',')\n",
        "        allSamples += Numpy2Dataset(notes)\n",
        "        if i % 100 == 0:\n",
        "            print(i)\n",
        "    \n",
        "    return allSamples\n",
        "\n",
        "def SaveSamplesTensor(samples, outputPath): # save tensor\n",
        "    tens = torch.Tensor(samples)\n",
        "    torch.save(samples, outputPath+\"Notes_Dataset.pt\")\n",
        "    return tens   \n",
        "\n",
        "def SaveAllSamples(dataPath, outputPath): # save dataset tensor\n",
        "    samples = SampleAllNumpy(dataPath)\n",
        "    SaveSamplesTensor(samples, outputPath)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0zg4NX2jjLz"
      },
      "source": [
        "#### Bulk data conversion code - COMMENT OUT IF NOT IN USE!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qRrA4ZkjjL0"
      },
      "source": [
        "#SaveAllSamples(\"data/numpy_files/\",\"data/\") #save all into tensor"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA3gNjzkjjL2"
      },
      "source": [
        "# IMPORTANT: COMMENT OUT IF NOT IN USE TO AVOID ACCIDENTS!!!!!!!\n",
        "\n",
        "# Getting CSVs from MIDI data and processed data from CSVs\n",
        "# Processed MIDI does not contain program messages and so are a good measure of what output SHOULD look like in a perfect world\n",
        "\n",
        "# dataPath = \"data/MIDI_files_original/\"\n",
        "# outputPath = \"data/numpy_files/\"\n",
        "# processedPath = \"data/MIDI_files_processed/\"\n",
        "\n",
        "# for i,f in enumerate(os.listdir(dataPath)):\n",
        "#     notes = Midi2Numpy(dataPath+f)\n",
        "#     np.savetxt(outputPath + \"MIDI_{:04d}.csv\".format(i),notes,delimiter=\",\")\n",
        "#     Numpy2Midi(notes, processedPath + \"MIDI_{:04d}\".format(i))\n",
        "    \n",
        "#     if i % 100 == 0:\n",
        "#         print(i)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzbQ3B1FjjL6"
      },
      "source": [
        "# dataPath = \"data/numpy_files/\"  # one-hot encoding on CSVs\n",
        "# outputPath = \"data/numpy_onehot\"\n",
        "\n",
        "# for i,f in enumerate(os.listdir(dataPath)):\n",
        "#     notes = np.genfromtxt(dataPath+f, delimiter=',')\n",
        "#     notes = NumpyDenormalize(notes)\n",
        "#     notes = NumpyOneHot(notes)\n",
        "#     notes = NumpyNormalize(notes, oneHot=True)\n",
        "#     np.savetxt(outputPath + \"MIDI_{:04d}.csv\".format(i),notes,delimiter=\",\")\n",
        "#     if i % 100 == 0:\n",
        "#         print(i)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HckUINzJjjL8"
      },
      "source": [
        "## Baseline Model Code\n",
        "#### getting available notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS_gJ_sljjL9"
      },
      "source": [
        "def GetAllNotesMajor(root):# Get all used notes in major scale of root=root\n",
        "    notes = []\n",
        "    intervals = [2,2,1,2,2,2,1]\n",
        "    \n",
        "    while root > 24: #bring down to lowest used octave\n",
        "        root -= 12\n",
        "    \n",
        "    n = root\n",
        "    notes.append(n)\n",
        "    while n < 84: #up to higherst used note\n",
        "        for i in intervals:\n",
        "            n += i\n",
        "            notes.append(n)   \n",
        "    return notes    \n",
        "\n",
        "\n",
        "def GetRangeMajor(notes, low, high): # Get all notes within range\n",
        "    lowIndex = notes.index(low)\n",
        "    highIndex = notes.index(high)\n",
        "    \n",
        "    return notes[lowIndex:highIndex+1]   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMKnIlZtjjL_"
      },
      "source": [
        "#### Piece Class\n",
        "##### represents whole output from all 4 voices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egw2ow53jjMA"
      },
      "source": [
        "class Piece: # Entire baseline model compostion - composed of 4 voices soprano, alto, tenor, bass (SATB)\n",
        "    def __init__(self, barNum=16, root=60):# 16 bars in C major\n",
        "        self.root = root # root note\n",
        "        self.allNotes = GetAllNotesMajor(self.root) # all notes on major scale\n",
        "        self.barNum = barNum # number of bars\n",
        "        \n",
        "        self.soprano = Voice(self.allNotes,60,84,speed=8) # SATB\n",
        "        self.alto = Voice(self.allNotes,48,72)\n",
        "        self.tenor = Voice(self.allNotes,36,60)\n",
        "        self.bass = Voice(self.allNotes,24,48)\n",
        "          \n",
        "        self.notes = np.empty([0,4]) #notes output\n",
        "        \n",
        "        self.pieceChords = [] # chords\n",
        "        \n",
        "        self.chords = np.array([ # common classical C major chords\n",
        "            [ 0,  4,  7,  0],# I\n",
        "            [ 2,  5,  9,  2],# ii\n",
        "            [ 4,  7, 11,  4],# iii\n",
        "            [ 5,  9, 0,  5],# IV\n",
        "            [ 7, 11, 2,  7],# V\n",
        "            [ 9, 0, 4,  9],# vi\n",
        "            [11, 2, 5, 11],# vii dim\n",
        "            [ 2,  5,  9, 0],# ii7\n",
        "            [ 5,  9, 0, 4],# IVmaj7\n",
        "            [ 7, 11, 2, 5],# V7\n",
        "            [11, 2, 5, 9]])# vii7 half-dim\n",
        "        \n",
        "    def GenerateSoprano(self): # Generate soprano line\n",
        "        self.soprano.GenerateLine(self.soprano.speed*self.barNum)\n",
        "        \n",
        "    def GenerateAlto(self): # Generate alto line from chords\n",
        "        self.alto.GenerateChordLine(self.pieceChords)\n",
        "        \n",
        "    def GenerateTenor(self): # see alto\n",
        "        self.tenor.GenerateChordLine(self.pieceChords)\n",
        "        \n",
        "    def GenerateBass(self): # see alto\n",
        "        self.bass.GenerateChordLine(self.pieceChords)\n",
        "        \n",
        "        \n",
        "    \n",
        "    def ChooseChord(self, sopNote): # Choose a fitting chord for soprano note\n",
        "        while sopNote >= 12:\n",
        "            sopNote -= 12\n",
        "        \n",
        "        goodChords = np.empty([0,4])\n",
        "        \n",
        "        for chord in self.chords:\n",
        "            if (chord==sopNote).sum() > 0:\n",
        "                goodChords = np.append(goodChords,[chord],axis=0)\n",
        "        \n",
        "        chosenChord = goodChords[random.randint(0,len(goodChords)-1)]\n",
        "        chosenChord = np.sort(np.unique(chosenChord))\n",
        "        \n",
        "        i = 12\n",
        "        chordNotes = chosenChord\n",
        "        while i < 120:\n",
        "            chordNotes = np.append(chordNotes, chosenChord+i)\n",
        "            i += 12\n",
        "        \n",
        "        return(chordNotes)\n",
        "    \n",
        "    def GetChords(self): # select all chords in piece\n",
        "        for i, note in enumerate(self.soprano.notes):\n",
        "            if i % 2 == 0:\n",
        "                sopNote = note[0]\n",
        "                chord = self.ChooseChord(sopNote)\n",
        "                self.pieceChords.append(chord)\n",
        "                \n",
        "    def Normalize(self): # normalize all values to 0-1\n",
        "        for i, msg in enumerate(self.notes):\n",
        "            self.notes[i,0] = msg[0]/128\n",
        "            self.notes[i,1] = msg[1]/128\n",
        "            self.notes[i,2] = msg[2]/40000\n",
        "            self.notes[i,3] = msg[3]/40000\n",
        "                \n",
        "    def GenerateLines(self): # Generate all SATB lines and joins them - entire baseline model\n",
        "        self.GenerateSoprano()\n",
        "        self.GetChords()\n",
        "        self.GenerateAlto()\n",
        "        self.GenerateTenor()\n",
        "        self.GenerateBass()\n",
        "        self.joinLines()\n",
        "        self.OffsetTime(20)\n",
        "        self.Normalize()\n",
        "        \n",
        "        return self.notes\n",
        "        \n",
        "    def InsertLine(self, starting, inserted, startIndex, skipIndex): # join 2 lines\n",
        "        base = np.copy(starting)\n",
        "        ins = np.copy(inserted)\n",
        "        \n",
        "        for i,note in enumerate(ins):\n",
        "            base = np.insert(base, (i*skipIndex)+startIndex, [note], axis=0)\n",
        "            \n",
        "        return base\n",
        "        \n",
        "    def joinLines(self): # join all SATB lines\n",
        "        #self.notes = np.copy(self.soprano)\n",
        "        self.notes = self.InsertLine(self.soprano.notes, self.alto.notes, 1, 3)\n",
        "        self.notes = self.InsertLine(self.notes, self.tenor.notes, 2, 4)\n",
        "        self.notes = self.InsertLine(self.notes, self.bass.notes, 3, 5)\n",
        "        \n",
        "    def OffsetTime(self, maxChange): # adds random time offsets to make output sound more organic\n",
        "        for note in self.notes:\n",
        "            note[2] += random.randint(0,maxChange)\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCKtb0UjjMD"
      },
      "source": [
        "#### Voice class\n",
        "##### Represents individual voices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ftdfULjjME"
      },
      "source": [
        "class Voice: # individual voices\n",
        "    def __init__(self, allNotes, lowNote, highNote, jump=3, speed=4, time=4096, velocity=64):\n",
        "        self.range = GetRangeMajor(allNotes,lowNote,highNote) #available ntoes\n",
        "        self.jump = jump #maximum pitch interval between notes\n",
        "        self.speed = speed #note length i.e. 4 for quarter, 8 for eighth etc.\n",
        "        self.time = time #song speed\n",
        "        self.velocity = velocity #note volume\n",
        "        self.notes = np.empty([0,4]) #notes output\n",
        "        self.lowNote = lowNote # lowest note\n",
        "        self.highNote = highNote # highest note\n",
        "        self.allNotes = allNotes # all notes in scale\n",
        "            \n",
        "        self.duration = self.time / self.speed # time between notes\n",
        "        \n",
        "        \n",
        "    def RandomStartNote(self): # Generate Random first note (for soprano)\n",
        "        note = random.choice(self.range)\n",
        "        self.notes = np.append(self.notes,np.array([[note, self.velocity, 0, self.duration]]),axis=0)\n",
        "        \n",
        "        \n",
        "    def RandomJump(self): # Generate Random next note (for soprano)\n",
        "        lastNote = self.notes[len(self.notes)-1][0] # find last played note\n",
        "        lastIndex = self.range.index(lastNote)\n",
        "        \n",
        "        newIndex = -1\n",
        "        while newIndex < 0 or newIndex >= len(self.range): # stay in range\n",
        "            newIndex = lastIndex + random.randint(-self.jump,self.jump)\n",
        "            \n",
        "        newNote = self.range[newIndex]\n",
        "        self.notes = np.append(self.notes,np.array([[newNote, self.velocity, self.duration, self.duration]]),axis=0)\n",
        "        \n",
        "        \n",
        "    def GenerateLine(self, length): # Generate random line (for soprano)\n",
        "        self.RandomStartNote()\n",
        "        \n",
        "        for n in range(length-1):\n",
        "            self.RandomJump()\n",
        "            \n",
        "            \n",
        "    def clearNotes(self):\n",
        "        self.notes = np.empty([0,4])\n",
        "        \n",
        "    def GetChordNotes(self, chordNotes): # Get useful notes from all chord notes\n",
        "        chordNotes = chordNotes[chordNotes >= self.lowNote]\n",
        "        chordNotes = chordNotes[chordNotes <= self.highNote]\n",
        "        return chordNotes\n",
        "    \n",
        "    def ChooseStartChordNote(self, chordNotes): # Choose Random note in chord\n",
        "        note = random.choice(chordNotes)\n",
        "        self.notes = np.append(self.notes,np.array([[note, self.velocity, 0, self.duration]]),axis=0)\n",
        "        \n",
        "    def ChooseChordNote(self,chordNotes): # Choose suitable next note in chord\n",
        "        lastNote = self.notes[len(self.notes)-1][0] # find last played note\n",
        "        \n",
        "        chordNotes = chordNotes[chordNotes >= lastNote - (self.jump*2)]\n",
        "        chordNotes = chordNotes[chordNotes <= lastNote + (self.jump*2)]\n",
        "        newNote = random.choice(chordNotes)\n",
        "        \n",
        "        self.notes = np.append(self.notes,np.array([[newNote, self.velocity, 0, self.duration]]),axis=0)\n",
        "        \n",
        "    def GenerateChordLine(self, chords): # Generate A/T/B lines\n",
        "        \n",
        "        firstChord = self.GetChordNotes(chords[0])\n",
        "        self.ChooseStartChordNote(firstChord)\n",
        "        \n",
        "        for c in chords[1:]:\n",
        "            chord = self.GetChordNotes(c)\n",
        "            self.ChooseChordNote(chord)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSYLfWz-jjMK"
      },
      "source": [
        "## Model architecture definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e54Jnk9djjMK"
      },
      "source": [
        "Set the hyperparameters below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJCrUv0ww-F"
      },
      "source": [
        "## Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvkryl71-FCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3505b502-13ac-46e1-a190-64fa213e2e3d"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, n_features, emb_dim, num_layers, dropout):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(n_features, emb_dim, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        return hn\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, n_features, emb_dim, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.num_layers, self.hidden_dim = num_layers, 2*emb_dim\n",
        "        self.lstm = nn.LSTM(emb_dim, self.hidden_dim, num_layers, batch_first=True,\n",
        "                            dropout=dropout)\n",
        "        self.fc = torch.nn.Linear(self.hidden_dim, n_features)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = torch.sigmoid(self.fc(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class LSTMAutoEncoder(nn.Module):\n",
        "    def __init__(self, n_features, emb_dim, num_layers, dropout, batch_size):\n",
        "        super(LSTMAutoEncoder, self).__init__()\n",
        "        self.name = \"LSTMAutoEncoder\"\n",
        "        self.encoder = LSTMEncoder(n_features, emb_dim, num_layers, dropout)\n",
        "        self.decoder = LSTMDecoder(n_features, emb_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_x = self.encoder(x)\n",
        "        decoded_x = self.decoder(encoded_x)\n",
        "        return decoded_x[-1]\n",
        "\n",
        "print('Model class created succesfully')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model class created succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T63ukNsTjjMU"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9CtbU76jjMU"
      },
      "source": [
        "#To help us save the model easier...\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ITaiqxl3XBL"
      },
      "source": [
        "def get_accuracy(model, data): #Accuracy on note selection...\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sample in data:\n",
        "        excerpt = sample[:,:-1,:] # Extracts all but the last row (model will predict last note)\n",
        "        true_note = sample[:,-1,:].detach().numpy() # Extracts the last row (what we want it to predict)\n",
        "        pred_note = model(excerpt).detach().numpy()\n",
        "\n",
        "        diff = np.abs(pred_note - true_note)\n",
        "        correct = len(diff[np.where(diff <= 1e-2)])\n",
        "        total = pred_note.shape[0]*pred_note.shape[1]\n",
        "    return correct / total"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU9kZTAyKWI6"
      },
      "source": [
        "def fit(model, train_loader, criterion, num_epochs, batch_size, learning_rate, class_regr):\n",
        "    losses, accuracy = [], []\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate, \n",
        "                                 weight_decay=1e-5) # <-- Sometimes Adam converges faster than SGD\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, data in enumerate(train_loader):\n",
        "            excerpt = data[:,:-1] # Extracts all but the last row \n",
        "            true_note = data[:,-1]  # Extracts the last row (what we want it to predict)\n",
        "            out = model(excerpt)             # forward pass\n",
        "\n",
        "            if class_regr == \"classify\":\n",
        "                # Use classification for Note\n",
        "                pred = out[:,:12]\n",
        "                target = np.argmax(true_note[:,:12], axis=1)\n",
        "            else:\n",
        "                # Regression - Octave, Velocity, Time, Sustain\n",
        "                pred = out[:,12:]\n",
        "                target = true_note[:,12:]\n",
        "\n",
        "            loss = criterion(pred, target) # compute the total loss\n",
        "\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "        \n",
        "        accurate = get_accuracy(model, train_loader)\n",
        "        accuracy.append(accurate)\n",
        "        \n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
        "        #Checkpoint the model every epoch\n",
        "        model_path = get_model_name(model.name, batch_size, learning_rate, epoch) #Returns the model name for \n",
        "        #the save file.\n",
        "        torch.save(model.state_dict(), model_path) #Saves the current model with the weights.\n",
        "    return losses, accuracy"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ruVOFo5PwQ6"
      },
      "source": [
        "def train(model_classify, model_regr, train_data, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
        "    torch.manual_seed(1000) #Fixed. Make sure we use this throughout...\n",
        "    criterion_classfy = nn.CrossEntropyLoss()\n",
        "    criterion_regr = nn.MSELoss()\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "    \n",
        "    n = 0 # the number of iterations\n",
        "    start_time=time.time() #Start of training\n",
        "\n",
        "    # Classification - Note\n",
        "    loss_classify, accuracy_classify = fit(model_classify, train_loader, criterion_classfy,\n",
        "                        num_epochs, batch_size, learning_rate, class_regr=\"classify\")\n",
        "    # Regression - Octave, Velocity, Time, Sustain\n",
        "    loss_regr, accuracy_regr = fit(model_regr, train_loader, criterion_regr,\n",
        "                    num_epochs, batch_size, learning_rate, class_regr=\"regression\")\n",
        "\n",
        "    end_time= time.time()\n",
        "\n",
        "    return loss_classify, accuracy_classify, loss_regr, accuracy_regr"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQqCUe3mjjMj"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGaCHzIUjjMk"
      },
      "source": [
        "# Load saved tensor dataset\n",
        "data = torch.load(r'/content/gdrive/My Drive/APS360/Project/Notes_Dataset.pt')\n",
        "\n",
        "# Prevent type errors\n",
        "data = torch.tensor(data).float()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64sbdhl9_lJo"
      },
      "source": [
        "def Get88NoteOneHot(note):\n",
        "    notes = np.copy(note)\n",
        "    oneHot = np.zeros([len(notes),16])\n",
        "    oneHot[:, 13:] = notes[:, 1:]\n",
        "    # The lowest pitch in piano roll\n",
        "    min = 21\n",
        "    names = notes[:,0]-min\n",
        "    namesOct = names%12\n",
        "    oneHot[:,12] = (names-(namesOct))/12\n",
        "    \n",
        "    for i, name in enumerate(namesOct):\n",
        "        oneHot[i,name.astype(int)] = 1\n",
        "    \n",
        "    # if a note is played later than 8 seconds after, just set it to 8 seconds\n",
        "    oneHot[:,14][(oneHot[:,14] > 960*8)] = 960*8\n",
        "    # if a note is for longer than 8 seconds, just set it to 8 seconds\n",
        "    oneHot[:,15][(oneHot[:,15] > 960*8)] = 960*8\n",
        "\n",
        "    oneHot[:,12] /= 7\n",
        "    oneHot[:,13] /= 128\n",
        "    oneHot[:,14] /= 960*8\n",
        "    oneHot[:,15] /= 960*8\n",
        "\n",
        "    return oneHot"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EWE2ibcoIf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfef54b-bc08-4599-c89d-7cdf8fdbe1b7"
      },
      "source": [
        "# Split into smaller dataset for training\n",
        "train_data = data[:40]\n",
        "test_train_data = train_data.view(-1, train_data.shape[-1]).detach().numpy()\n",
        "\n",
        "denorm = NumpyDenormalize(test_train_data)\n",
        "note_onehot = Get88NoteOneHot(denorm)\n",
        "\n",
        "print(note_onehot.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(800, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKt4LLEFzJKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb6189c-ae80-4139-c83d-1bf9f7b9a8aa"
      },
      "source": [
        "sample_len = 11\n",
        "train_samples = []\n",
        "for i in range(len(note_onehot) - sample_len):\n",
        "\t# grab from i to i + sample_len\n",
        "\tsample = note_onehot[i:i+sample_len]\n",
        "\ttrain_samples.append(sample)\n",
        " \n",
        "train_samples = np.array(train_samples)\n",
        "train_samples = torch.tensor(train_samples).float()\n",
        "print(train_samples.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([789, 11, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDKXrkkxrh0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abff3c7-ac4f-4762-9d62-e44f30e7bb2e"
      },
      "source": [
        "# For LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html #torch.nn.LSTM \n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "N_FEATURES = 16\n",
        "N_LAYERS = 2\n",
        "EMB_DIM = 64\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# Train the model\n",
        "model_classify = LSTMAutoEncoder(N_FEATURES, EMB_DIM, N_LAYERS, DROPOUT, BATCH_SIZE)\n",
        "model_regr = LSTMAutoEncoder(N_FEATURES, EMB_DIM, N_LAYERS, DROPOUT, BATCH_SIZE)\n",
        "\n",
        "loss_classify, accuracy_classify, loss_regr, accuracy_regr = train(model_classify, model_regr, train_samples, num_epochs=30, batch_size=BATCH_SIZE, learning_rate=0.001)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Loss:2.4650\n",
            "Epoch:2, Loss:2.4283\n",
            "Epoch:3, Loss:2.4283\n",
            "Epoch:4, Loss:2.4229\n",
            "Epoch:5, Loss:2.4181\n",
            "Epoch:6, Loss:2.4158\n",
            "Epoch:7, Loss:2.4130\n",
            "Epoch:8, Loss:2.4121\n",
            "Epoch:9, Loss:2.4112\n",
            "Epoch:10, Loss:2.4097\n",
            "Epoch:11, Loss:2.4099\n",
            "Epoch:12, Loss:2.4070\n",
            "Epoch:13, Loss:2.4066\n",
            "Epoch:14, Loss:2.4054\n",
            "Epoch:15, Loss:2.3987\n",
            "Epoch:16, Loss:2.3979\n",
            "Epoch:17, Loss:2.3989\n",
            "Epoch:18, Loss:2.4001\n",
            "Epoch:19, Loss:2.3983\n",
            "Epoch:20, Loss:2.3988\n",
            "Epoch:21, Loss:2.3965\n",
            "Epoch:22, Loss:2.3968\n",
            "Epoch:23, Loss:2.3967\n",
            "Epoch:24, Loss:2.3935\n",
            "Epoch:25, Loss:2.3936\n",
            "Epoch:26, Loss:2.3663\n",
            "Epoch:27, Loss:2.3179\n",
            "Epoch:28, Loss:2.3123\n",
            "Epoch:29, Loss:2.2746\n",
            "Epoch:30, Loss:2.2409\n",
            "Epoch:1, Loss:0.0247\n",
            "Epoch:2, Loss:0.0071\n",
            "Epoch:3, Loss:0.0055\n",
            "Epoch:4, Loss:0.0048\n",
            "Epoch:5, Loss:0.0048\n",
            "Epoch:6, Loss:0.0048\n",
            "Epoch:7, Loss:0.0048\n",
            "Epoch:8, Loss:0.0049\n",
            "Epoch:9, Loss:0.0050\n",
            "Epoch:10, Loss:0.0049\n",
            "Epoch:11, Loss:0.0049\n",
            "Epoch:12, Loss:0.0050\n",
            "Epoch:13, Loss:0.0051\n",
            "Epoch:14, Loss:0.0049\n",
            "Epoch:15, Loss:0.0048\n",
            "Epoch:16, Loss:0.0048\n",
            "Epoch:17, Loss:0.0048\n",
            "Epoch:18, Loss:0.0048\n",
            "Epoch:19, Loss:0.0048\n",
            "Epoch:20, Loss:0.0049\n",
            "Epoch:21, Loss:0.0048\n",
            "Epoch:22, Loss:0.0049\n",
            "Epoch:23, Loss:0.0049\n",
            "Epoch:24, Loss:0.0048\n",
            "Epoch:25, Loss:0.0048\n",
            "Epoch:26, Loss:0.0048\n",
            "Epoch:27, Loss:0.0048\n",
            "Epoch:28, Loss:0.0049\n",
            "Epoch:29, Loss:0.0048\n",
            "Epoch:30, Loss:0.0049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_nZ6XtLjjMn"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGp1T_uujjMo"
      },
      "source": [
        "Since our model is 'tested' with people listening to it, we need to just generate some samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSCP77Imdk8w"
      },
      "source": [
        "# Split into smaller dataset for training\n",
        "test_data = data[:100].detach().numpy()\n",
        "\n",
        "for i, sample in enumerate(test_data):\n",
        "    if i == 0:\n",
        "        all_test_data = sample\n",
        "    else:\n",
        "        all_test_data = np.append(all_test_data, sample, axis=0)\n",
        "\n",
        "test_denorm = NumpyDenormalize(all_test_data)\n",
        "test_note_onehot = Get88NoteOneHot(test_denorm)\n",
        "\n",
        "sample_len = 11\n",
        "test_samples = []\n",
        "for i in range(len(test_note_onehot) - sample_len):\n",
        "\t# grab from i to i + sample_len\n",
        "\tsample = test_note_onehot[i:i+sample_len]\n",
        "\ttest_samples.append(sample)\n",
        " \n",
        "test_samples = np.array(test_samples)\n",
        "test_samples = torch.tensor(test_samples).float()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytfQLyq1MHBF"
      },
      "source": [
        "def GetNormalizedOutput(note):\n",
        "    notes = np.copy(note)\n",
        "\n",
        "    minNote = 21\n",
        "    names = np.argmax(notes[:,:12], axis = 1)\n",
        "    octave = np.round((notes[:,12]*7))\n",
        "    octave_offset = 12*octave\n",
        "    notes[:,12] = (names + octave_offset + minNote)/128\n",
        "    notes[:,14] *= (960*8/40000)\n",
        "    notes[:,15] *= (960*8/40000)\n",
        "\n",
        "    return notes[:,12:]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgxPwKl9jjMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026f4241-4280-4540-b337-ba6d4a98877f"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_samples, \n",
        "                                           batch_size=1, \n",
        "                                           shuffle=True)\n",
        "\n",
        "song_length = 50\n",
        "\n",
        "for i, sample in enumerate(test_loader):\n",
        "    # Get note from classification model\n",
        "    output_note = model_classify(sample)[0].detach().numpy()\n",
        "    # Get other features (octave, velocity, time, sustain) from regression model\n",
        "    output_other = model_regr(sample)[0].detach().numpy()\n",
        "\n",
        "    # Combine the two outputs\n",
        "    output = np.append(output_note[:12], output_other[12:])\n",
        "    new_sample = np.expand_dims(output, axis=0)\n",
        "\n",
        "    if i == 0:\n",
        "        new_song = new_sample\n",
        "    else:\n",
        "        new_song = np.append(new_song, new_sample, axis=0)\n",
        "    if len(new_song) >= song_length:\n",
        "        break\n",
        "\n",
        "new_song = GetNormalizedOutput(new_song)\n",
        "\n",
        "# new_excerpt = new_excerpt.type(torch.int64)\n",
        "print('new_song: (Watch for the same notes appearing...)',new_song)\n",
        "print('new_song.shape: ',new_song.shape)\n",
        "\n",
        "mid = Numpy2Midi(new_song, \"Autoencoder\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new_song: (Watch for the same notes appearing...) [[0.515625   0.47161615 0.01534053 0.02393847]\n",
            " [0.4921875  0.4708471  0.0164914  0.02399817]\n",
            " [0.515625   0.46877015 0.01679269 0.02397291]\n",
            " [0.515625   0.47043896 0.01638414 0.02427922]\n",
            " [0.4921875  0.4657025  0.0204136  0.02753453]\n",
            " [0.515625   0.4686741  0.01738581 0.025387  ]\n",
            " [0.515625   0.4726493  0.01560819 0.02421252]\n",
            " [0.515625   0.4682168  0.01615406 0.02424193]\n",
            " [0.4921875  0.4725493  0.01963989 0.02789339]\n",
            " [0.515625   0.47048658 0.01692159 0.02474781]\n",
            " [0.4921875  0.46667624 0.01719424 0.02473126]\n",
            " [0.515625   0.472897   0.0153676  0.02329619]\n",
            " [0.515625   0.47229734 0.01472211 0.02244608]\n",
            " [0.515625   0.47279614 0.0160325  0.0239346 ]\n",
            " [0.515625   0.4695932  0.01791228 0.02569024]\n",
            " [0.515625   0.4640951  0.01625742 0.0237198 ]\n",
            " [0.4921875  0.47062704 0.01520271 0.02268414]\n",
            " [0.515625   0.46943563 0.0162066  0.02399646]\n",
            " [0.515625   0.46803594 0.01989923 0.02778774]\n",
            " [0.515625   0.46935043 0.01714952 0.02411349]\n",
            " [0.4921875  0.4714258  0.01565925 0.0236503 ]\n",
            " [0.515625   0.46930683 0.01724465 0.02501657]\n",
            " [0.515625   0.46688855 0.01685533 0.02468484]\n",
            " [0.515625   0.47099155 0.01649925 0.0239173 ]\n",
            " [0.515625   0.46645567 0.01619147 0.02388152]\n",
            " [0.515625   0.47335565 0.01619162 0.02389596]\n",
            " [0.4921875  0.47471625 0.01570245 0.02328928]\n",
            " [0.515625   0.47089198 0.01757302 0.0251321 ]\n",
            " [0.515625   0.47431785 0.01727394 0.02496618]\n",
            " [0.4921875  0.46992022 0.01664643 0.02507235]\n",
            " [0.515625   0.4652717  0.01652243 0.02382533]\n",
            " [0.4921875  0.47534904 0.01883322 0.02605574]\n",
            " [0.515625   0.46329278 0.01521777 0.02245021]\n",
            " [0.4921875  0.46582973 0.0175791  0.02432342]\n",
            " [0.515625   0.4747288  0.01614067 0.02326882]\n",
            " [0.515625   0.465603   0.01699023 0.02453899]\n",
            " [0.4921875  0.47907472 0.0186593  0.02615786]\n",
            " [0.515625   0.46876544 0.01537805 0.02311827]\n",
            " [0.515625   0.47193593 0.01792053 0.02503572]\n",
            " [0.4921875  0.47625905 0.01637409 0.02431721]\n",
            " [0.4921875  0.47075737 0.01560701 0.02315486]\n",
            " [0.515625   0.46726027 0.01570006 0.02342478]\n",
            " [0.515625   0.477924   0.01607619 0.02372478]\n",
            " [0.4921875  0.47003973 0.01678082 0.02434877]\n",
            " [0.515625   0.46990043 0.01708588 0.0256458 ]\n",
            " [0.515625   0.47102433 0.0163417  0.02403358]\n",
            " [0.4921875  0.4716299  0.0176759  0.02475597]\n",
            " [0.515625   0.46389008 0.0149394  0.0218911 ]\n",
            " [0.515625   0.47524342 0.01669346 0.02434803]\n",
            " [0.4921875  0.47006926 0.01639692 0.02429755]]\n",
            "new_song.shape:  (50, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}