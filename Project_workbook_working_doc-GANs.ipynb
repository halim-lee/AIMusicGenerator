{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df5TDjFL82_D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey6eRWKT82_L"
   },
   "source": [
    "# Project File - APS360 Team 25\n",
    "Divided into the following section: \n",
    "# \n",
    "1) Library imports\n",
    "2) Data imports\n",
    "3) Model architecture definition\n",
    "4) Training function definition\n",
    "5) Model training\n",
    "6) Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezUOY2hQ82_L"
   },
   "source": [
    "## Library imports \n",
    "(Place all library imports here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rh5tkXcn82_M",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mF33UXrn82_R"
   },
   "outputs": [],
   "source": [
    "#KP - I just added the main ones from the labs.\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time # Tracking model training time.\n",
    "\n",
    "#for Data importing\n",
    "import mido\n",
    "from mido import MidiFile, Message, MidiTrack, MetaMessage\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5ih3-feE82_V"
   },
   "outputs": [],
   "source": [
    "#Set working directory if required:\n",
    "os.chdir('D:\\engsci\\year 3\\CLASS\\APS360\\Project') #Sets current working directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q63JUNja82_Z"
   },
   "source": [
    "## Data imports\n",
    "#### MIDI reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k1mc7s8X82_Z"
   },
   "outputs": [],
   "source": [
    "def CountTracks(directory):          #Count files and tracks in folder\n",
    "    trackCount = 0\n",
    "    fileCount = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".midi\"):\n",
    "            fileCount += 1\n",
    "            midiDir = MidiFile(directory+\"/\"+file)\n",
    "            for track in midiDir.tracks:\n",
    "                trackCount += 1\n",
    "    print(fileCount+\" files\")\n",
    "    print(trackCount+\" tracks\")\n",
    "\n",
    "    \n",
    "def PrintMessages(mid):                # print midi messages\n",
    "    for i, track in enumerate(mid.tracks):\n",
    "        print('Track {}: {}'.format(i, track.name))\n",
    "        for msg in track:\n",
    "            print(msg)\n",
    "\n",
    "            \n",
    "def PrintSomeMessages(mid):             #print first 200 midi messages\n",
    "    track = mid.tracks[1]\n",
    "    for i,msg in enumerate(track):\n",
    "        if i < 200:\n",
    "            print(msg)\n",
    "            \n",
    "def PrintMetaMessages(mid):             #print fmeta messages\n",
    "    track = mid.tracks[0]\n",
    "    for i,msg in enumerate(track):\n",
    "        print(msg)\n",
    "\n",
    "def cleanupMessages(mid):              #removes non-note messages by force\n",
    "    track = mid.tracks[1]\n",
    "    track2 = []\n",
    "    for msg in track:\n",
    "        if msg.type == \"note_on\":\n",
    "            track2.append(msg)\n",
    "    mid.tracks[1] = track2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NflNskX682_c"
   },
   "source": [
    "#### MIDI to Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J5f1ieE182_c"
   },
   "outputs": [],
   "source": [
    "\n",
    "def Midi2NumpyNoSustain(mid):                                #converts to numpy array removing non-note messages\n",
    "    track = mid.tracks[1]                           #0th track only contains meta-messages, all notes on 1st track\n",
    "    notes = np.empty([0,4])\n",
    "    time = 0\n",
    "    for msg in track:\n",
    "        if msg.type == \"note_on\":                   # only count \"note\" messages - other inputs i.e. foot pedals are ignored\n",
    "            notes = np.append(notes,np.array([[msg.note, msg.velocity, msg.time + time, 0]]),axis=0)         # (note, velocity, time, sustain)\n",
    "            time = 0\n",
    "        else:\n",
    "            time += msg.time                        #adjust time when removing other messages\n",
    "    return notes\n",
    "\n",
    "\n",
    "def NumpyGetSustain(note):\n",
    "    notes = np.copy(note)\n",
    "    for i, msg in enumerate(notes):\n",
    "        if msg[1] > 0:                            # if velocity is not 0\n",
    "            j = 1\n",
    "            sustain = 0\n",
    "            while msg[0] != notes[i+j][0]:        # while note values are different\n",
    "                sustain += notes[i+j][2]\n",
    "                j += 1                            #search for next message with same note i.e. message telling that note was released\n",
    "            notes[i,3] = sustain + notes[i+j][2]\n",
    "    time = 0\n",
    "    for i, msg in enumerate(notes):\n",
    "        if msg[1] > 0:\n",
    "            notes[i,2] += time\n",
    "            time = 0\n",
    "        else:\n",
    "            time += msg[2]                        #adjust time\n",
    "    notes = notes[notes[:,1] > 0]                 #filter for notes with positive velocities (note presses)\n",
    "    return notes\n",
    "\n",
    "def NumpyNormalize(note, oneHot=False):                         #normalize all values to 0-1\n",
    "    notes = np.copy(note)\n",
    "    \n",
    "    if oneHot:\n",
    "        notes[:,12] /= 11\n",
    "        notes[:,13] /= 128\n",
    "        notes[:,14] /= 40000\n",
    "        notes[:,15] /= 40000\n",
    "    else:\n",
    "        notes[:,0] /= 128\n",
    "        notes[:,1] /= 128\n",
    "        notes[:,2] /= 40000\n",
    "        notes[:,3] /= 40000       \n",
    "    return notes\n",
    "\n",
    "def NumpyOneHot(note):\n",
    "    notes = np.copy(note)\n",
    "    oneHot = np.zeros([len(notes),16])\n",
    "    oneHot[:, 13:] = notes[:, 1:]\n",
    "    names = notes[:,0]\n",
    "    namesOct = names%12\n",
    "    oneHot[:,12] = (names-(namesOct))/12\n",
    "    \n",
    "    for i, name in enumerate(namesOct):\n",
    "        oneHot[i,name.astype(int)] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "def Midi2Numpy(path, oneHot=False): # full midi to numpy conversion\n",
    "    mid = MidiFile(path)\n",
    "    notes = Midi2NumpyNoSustain(mid)\n",
    "    cleanNotes = NumpyGetSustain(notes)\n",
    "    \n",
    "    if oneHot:\n",
    "        cleanNotes = NumpyOneHot(cleanNotes)\n",
    "    \n",
    "    normNotes = NumpyNormalize(cleanNotes, oneHot=oneHot)\n",
    "    return normNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv1SUW0P82_f"
   },
   "source": [
    "#### Numpy to MIDI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bGg9Tq3f82_g"
   },
   "outputs": [],
   "source": [
    "\n",
    "def NumpyDenormalize(note): # interpret all values from 0-1 to normal values\n",
    "    notes = np.copy(note)    \n",
    "    if notes.shape[1] == 16: # if encode as one-hot\n",
    "        notes[:,12] *= 11\n",
    "        notes[:,13] *= 128\n",
    "        notes[:,14] *= 40000\n",
    "        notes[:,15] *= 40000\n",
    "        \n",
    "        notes = NumpyEncode(notes) #encode back as original 4-variable format\n",
    "    else:\n",
    "        notes[:,0] *= 128\n",
    "        notes[:,1] *= 128\n",
    "        notes[:,2] *= 40000\n",
    "        notes[:,3] *= 40000       \n",
    "    return notes.astype(int)\n",
    "\n",
    "def NumpyEncode(note): # convert back from one-hot encoding\n",
    "    notes = np.copy(note)\n",
    "    encoded = np.zeros([len(notes),4])\n",
    "    encoded[:, 1:] = notes[:, 13:]\n",
    "    encoded[:, 0] = notes[:,12]*12\n",
    "    \n",
    "    for i in range(len(notes)):\n",
    "        encoded[i,0] += np.argmax(notes[i,:12])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def NumpySequence(notes): # put all notes into a \"timeline\" i.e.: time values of [10, 20, 10, 30] become [10, 30, 40, 70]\n",
    "    sequenced = np.copy(notes)                      # this allows us to easily add vel=0 notes in any order since we can later sort them by time\n",
    "    for i, msg in enumerate(sequenced):\n",
    "        if i > 0:\n",
    "            sequenced[i,2] += sequenced[i-1,2]\n",
    "    return sequenced\n",
    "\n",
    "def NumpyAddOffNotes(sequenced): # add vel=0 notes from sustain into sequenced timeline\n",
    "    withOff = np.copy(sequenced)\n",
    "    for msg in sequenced:\n",
    "        offNote = np.array([[msg[0], 0, msg[2] + msg[3], 0]])\n",
    "        withOff = np.append(withOff, offNote, axis=0)\n",
    "    #withOff = np.sort(withOff,axis=0)\n",
    "    withOff = withOff[withOff[:,2].argsort()] # sort by time\n",
    "    return withOff\n",
    "\n",
    "def NumpyUnsequence(notes): # revert time value to \"time since last message\"\n",
    "    unsequenced = np.copy(notes)\n",
    "    for i, msg in reversed(list(enumerate(unsequenced))):\n",
    "        unsequenced[i,3] = 0\n",
    "        if i > 0:\n",
    "            unsequenced[i,2] -= unsequenced[i-1,2]\n",
    "    return unsequenced\n",
    "\n",
    "def Numpy2MidiDirect(array):    #make MIDI object from numpy\n",
    "    #Start with initializing a new Mido Track:\n",
    "    mid = MidiFile()\n",
    "    track0 = MidiTrack()\n",
    "    track1 = MidiTrack()\n",
    "    \n",
    "    track0.append(MetaMessage('set_tempo', tempo=500000, time=0)) #MetaMessages not necessary but are present in used files\n",
    "    track0.append(MetaMessage('time_signature', numerator=4, denominator=4, clocks_per_click=24, notated_32nd_notes_per_beat=8, time=0))\n",
    "    track0.append(MetaMessage('end_of_track', time=1))\n",
    "    \n",
    "    track1.append(Message('program_change', channel=0, program=0, time=0))\n",
    "    \n",
    "    for i,note in enumerate(array):         # Get the index and the note. Array must be int array\n",
    "        j = 1\n",
    "        track1.append(Message('note_on',note = array[i,0], velocity = array[i,1],time = array[i,2])) # Add the note to the track.\n",
    "\n",
    "    mid.tracks.append(track0)\n",
    "    mid.tracks.append(track1)\n",
    "    return mid\n",
    "\n",
    "def Numpy2Midi(notes, name): # full numpy to midi conversion, saving result to [name].midi\n",
    "    denorm = NumpyDenormalize(notes)\n",
    "    seq = NumpySequence(denorm)\n",
    "    off = NumpyAddOffNotes(seq)\n",
    "    unseq = NumpyUnsequence(off)\n",
    "    mid = Numpy2MidiDirect(unseq)\n",
    "    mid.save(name + \".midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating tensor dataset from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Numpy2Dataset(notes,num=100,skip=10): # make list of sumpy arrays #Playing with window sized (num)\n",
    "    samples = []\n",
    "    i = 0\n",
    "    while i+num <= len(notes):\n",
    "        samples.append(notes[i:i+num])\n",
    "        i += skip\n",
    "    return samples\n",
    "\n",
    "def SampleAllNumpy(dataPath): # generate samples from all saved CSVs\n",
    "    allSamples = []\n",
    "\n",
    "    for i,f in enumerate(os.listdir(dataPath)):\n",
    "        \n",
    "        \n",
    "        if i % 1280 == 0:\n",
    "            print(i)\n",
    "            notes = np.genfromtxt(dataPath+f, delimiter=',') #Moved down... \n",
    "            allSamples += Numpy2Dataset(notes) #Moved down into if statement...\n",
    "    \n",
    "    return allSamples\n",
    "\n",
    "def SaveSamplesTensor(samples, outputPath): # save tensor\n",
    "    tens = torch.Tensor(samples)\n",
    "    torch.save(samples, outputPath+\"Notes_Dataset_test_onehot.pt\")\n",
    "    return tens   \n",
    "\n",
    "def SaveAllSamples(dataPath, outputPath): # save dataset tensor\n",
    "    samples = SampleAllNumpy(dataPath)\n",
    "    SaveSamplesTensor(samples, outputPath)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulk data conversion code - COMMENT OUT IF NOT IN USE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SaveAllSamples(\"D:/engsci/year 3/CLASS/APS360/Project/data/numpy_files/\",\"D:/engsci/year 3/CLASS/APS360/Project/data/\") #save all into tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdbiFWnW82_j"
   },
   "source": [
    "## Baseline Model Code\n",
    "#### getting available notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vU9vWH6j82_k"
   },
   "outputs": [],
   "source": [
    "def GetAllNotesMajor(root):# Get all used notes in major scale of root=root\n",
    "    notes = []\n",
    "    intervals = [2,2,1,2,2,2,1]\n",
    "    \n",
    "    while root > 24: #bring down to lowest used octave\n",
    "        root -= 12\n",
    "    \n",
    "    n = root\n",
    "    notes.append(n)\n",
    "    while n < 84: #up to higherst used note\n",
    "        for i in intervals:\n",
    "            n += i\n",
    "            notes.append(n)   \n",
    "    return notes    \n",
    "\n",
    "\n",
    "def GetRangeMajor(notes, low, high): # Get all notes within range\n",
    "    lowIndex = notes.index(low)\n",
    "    highIndex = notes.index(high)\n",
    "    \n",
    "    return notes[lowIndex:highIndex+1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtbCngxD82_m"
   },
   "source": [
    "#### Piece Class\n",
    "##### represents whole output from all 4 voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ohSiDLzv82_n"
   },
   "outputs": [],
   "source": [
    "class Piece: # Entire baseline model compostion - composed of 4 voices soprano, alto, tenor, bass (SATB)\n",
    "    def __init__(self, barNum=16, root=60):# 16 bars in C major\n",
    "        self.root = root # root note\n",
    "        self.allNotes = GetAllNotesMajor(self.root) # all notes on major scale\n",
    "        self.barNum = barNum # number of bars\n",
    "        \n",
    "        self.soprano = Voice(self.allNotes,60,84,speed=8) # SATB\n",
    "        self.alto = Voice(self.allNotes,48,72)\n",
    "        self.tenor = Voice(self.allNotes,36,60)\n",
    "        self.bass = Voice(self.allNotes,24,48)\n",
    "          \n",
    "        self.notes = np.empty([0,4]) #notes output\n",
    "        \n",
    "        self.pieceChords = [] # chords\n",
    "        \n",
    "        self.chords = np.array([ # common classical C major chords\n",
    "            [ 0,  4,  7,  0],# I\n",
    "            [ 2,  5,  9,  2],# ii\n",
    "            [ 4,  7, 11,  4],# iii\n",
    "            [ 5,  9, 0,  5],# IV\n",
    "            [ 7, 11, 2,  7],# V\n",
    "            [ 9, 0, 4,  9],# vi\n",
    "            [11, 2, 5, 11],# vii dim\n",
    "            [ 2,  5,  9, 0],# ii7\n",
    "            [ 5,  9, 0, 4],# IVmaj7\n",
    "            [ 7, 11, 2, 5],# V7\n",
    "            [11, 2, 5, 9]])# vii7 half-dim\n",
    "        \n",
    "    def GenerateSoprano(self): # Generate soprano line\n",
    "        self.soprano.GenerateLine(self.soprano.speed*self.barNum)\n",
    "        \n",
    "    def GenerateAlto(self): # Generate alto line from chords\n",
    "        self.alto.GenerateChordLine(self.pieceChords)\n",
    "        \n",
    "    def GenerateTenor(self): # see alto\n",
    "        self.tenor.GenerateChordLine(self.pieceChords)\n",
    "        \n",
    "    def GenerateBass(self): # see alto\n",
    "        self.bass.GenerateChordLine(self.pieceChords)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def ChooseChord(self, sopNote): # Choose a fitting chord for soprano note\n",
    "        while sopNote >= 12:\n",
    "            sopNote -= 12\n",
    "        \n",
    "        goodChords = np.empty([0,4])\n",
    "        \n",
    "        for chord in self.chords:\n",
    "            if (chord==sopNote).sum() > 0:\n",
    "                goodChords = np.append(goodChords,[chord],axis=0)\n",
    "        \n",
    "        chosenChord = goodChords[random.randint(0,len(goodChords)-1)]\n",
    "        chosenChord = np.sort(np.unique(chosenChord))\n",
    "        \n",
    "        i = 12\n",
    "        chordNotes = chosenChord\n",
    "        while i < 120:\n",
    "            chordNotes = np.append(chordNotes, chosenChord+i)\n",
    "            i += 12\n",
    "        \n",
    "        return(chordNotes)\n",
    "    \n",
    "    def GetChords(self): # select all chords in piece\n",
    "        for i, note in enumerate(self.soprano.notes):\n",
    "            if i % 2 == 0:\n",
    "                sopNote = note[0]\n",
    "                chord = self.ChooseChord(sopNote)\n",
    "                self.pieceChords.append(chord)\n",
    "                \n",
    "    def GenerateLines(self): # Generate all SATB lines and joins them - entire baseline model\n",
    "        self.GenerateSoprano()\n",
    "        self.GetChords()\n",
    "        self.GenerateAlto()\n",
    "        self.GenerateTenor()\n",
    "        self.GenerateBass()\n",
    "        self.joinLines()\n",
    "        \n",
    "        self.notes = self.notes.astype(int)\n",
    "        self.OffsetTime(20)\n",
    "        \n",
    "        return self.notes\n",
    "        \n",
    "    def InsertLine(self, starting, inserted, startIndex, skipIndex): # join 2 lines\n",
    "        base = np.copy(starting)\n",
    "        ins = np.copy(inserted)\n",
    "        \n",
    "        for i,note in enumerate(ins):\n",
    "            base = np.insert(base, (i*skipIndex)+startIndex, [note], axis=0)\n",
    "            \n",
    "        return base\n",
    "        \n",
    "    def joinLines(self): # join all SATB lines\n",
    "        #self.notes = np.copy(self.soprano)\n",
    "        self.notes = self.InsertLine(self.soprano.notes, self.alto.notes, 1, 3)\n",
    "        self.notes = self.InsertLine(self.notes, self.tenor.notes, 2, 4)\n",
    "        self.notes = self.InsertLine(self.notes, self.bass.notes, 3, 5)\n",
    "        \n",
    "    def OffsetTime(self, maxChange): # adds random time offsets to make output more organic\n",
    "        for note in self.notes:\n",
    "            note[2] += random.randint(0,maxChange)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOGUICCP82_q"
   },
   "source": [
    "#### Voice class\n",
    "##### Represents individual voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9Bepet6A82_q"
   },
   "outputs": [],
   "source": [
    "class Voice: # individual voices\n",
    "    def __init__(self, allNotes, lowNote, highNote, jump=3, speed=4, time=4096, velocity=64):\n",
    "        self.range = GetRangeMajor(allNotes,lowNote,highNote) #available ntoes\n",
    "        self.jump = jump #maximum pitch interval between notes\n",
    "        self.speed = speed #note length i.e. 4 for quarter, 8 for eighth etc.\n",
    "        self.time = time #song speed\n",
    "        self.velocity = velocity #note volume\n",
    "        self.notes = np.empty([0,4]) #notes output\n",
    "        self.lowNote = lowNote # lowest note\n",
    "        self.highNote = highNote # highest note\n",
    "        self.allNotes = allNotes # all notes in scale\n",
    "            \n",
    "        self.duration = self.time / self.speed # time between notes\n",
    "        \n",
    "        \n",
    "    def RandomStartNote(self): # Generate Random first note (for soprano)\n",
    "        note = random.choice(self.range)\n",
    "        self.notes = np.append(self.notes,np.array([[note, self.velocity, 0, self.duration]]),axis=0)\n",
    "        \n",
    "        \n",
    "    def RandomJump(self): # Generate Random next note (for soprano)\n",
    "        lastNote = self.notes[len(self.notes)-1][0] # find last played note\n",
    "        lastIndex = self.range.index(lastNote)\n",
    "        \n",
    "        newIndex = -1\n",
    "        while newIndex < 0 or newIndex >= len(self.range): # stay in range\n",
    "            newIndex = lastIndex + random.randint(-self.jump,self.jump)\n",
    "            \n",
    "        newNote = self.range[newIndex]\n",
    "        self.notes = np.append(self.notes,np.array([[newNote, self.velocity, self.duration, self.duration]]),axis=0)\n",
    "        \n",
    "        \n",
    "    def GenerateLine(self, length): # Generate random line (for soprano)\n",
    "        self.RandomStartNote()\n",
    "        \n",
    "        for n in range(length-1):\n",
    "            self.RandomJump()\n",
    "            \n",
    "            \n",
    "    def clearNotes(self):\n",
    "        self.notes = np.empty([0,4])\n",
    "        \n",
    "    def GetChordNotes(self, chordNotes): # Get useful notes from all chord notes\n",
    "        chordNotes = chordNotes[chordNotes >= self.lowNote]\n",
    "        chordNotes = chordNotes[chordNotes <= self.highNote]\n",
    "        return chordNotes\n",
    "    \n",
    "    def ChooseStartChordNote(self, chordNotes): # Choose Random note in chord\n",
    "        note = random.choice(chordNotes)\n",
    "        self.notes = np.append(self.notes,np.array([[note, self.velocity, 0, self.duration]]),axis=0)\n",
    "        \n",
    "    def ChooseChordNote(self,chordNotes): # Choose suitable next note in chord\n",
    "        lastNote = self.notes[len(self.notes)-1][0] # find last played note\n",
    "        \n",
    "        chordNotes = chordNotes[chordNotes >= lastNote - (self.jump*2)]\n",
    "        chordNotes = chordNotes[chordNotes <= lastNote + (self.jump*2)]\n",
    "        newNote = random.choice(chordNotes)\n",
    "        \n",
    "        self.notes = np.append(self.notes,np.array([[newNote, self.velocity, 0, self.duration]]),axis=0)\n",
    "        \n",
    "    def GenerateChordLine(self, chords): # Generate A/T/B lines\n",
    "        \n",
    "        firstChord = self.GetChordNotes(chords[0])\n",
    "        self.ChooseStartChordNote(firstChord)\n",
    "        \n",
    "        for c in chords[1:]:\n",
    "            chord = self.GetChordNotes(c)\n",
    "            self.ChooseChordNote(chord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuSg-HNn82_t"
   },
   "source": [
    "## Model architecture definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whyBQNP282_t"
   },
   "source": [
    "GANs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bj7P87Gu82_t"
   },
   "outputs": [],
   "source": [
    "sample_length = 100\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(16*sample_length, 1)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1) #Binary classification task with 1 output neuron. Could also do with 2.\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, sample_length*16) # flatten songs...\n",
    "        out = F.leaky_relu(self.fc1(x), 0.2) #Recall: learky_relu allows some negative values...\n",
    "#         x = self.dropout(x)\n",
    "#         x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "#         x = self.dropout(x)\n",
    "#         x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "#         x = self.dropout(x)\n",
    "#         out = self.fc4(x)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 128) #Encoding size hyperparameter...\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, 16*sample_length)\n",
    "        self.dropout = nn.Dropout(0.3) #Note the use of dropout.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        out = torch.sigmoid(self.fc4(x)) #Applies sigmoid to output. Ensures all values >= 0\n",
    "        out = out.reshape(-1,sample_length,16)\n",
    "        return out\n",
    "\n",
    "D = Discriminator()\n",
    "G = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJBevRrq82_2"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "x5yE0ez782_3"
   },
   "outputs": [],
   "source": [
    "#To help us save the model easier...\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DguAOKB082_5"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(G, D, lr=0.002, batch_size=64, num_epochs=20):\n",
    "\n",
    "\n",
    "\n",
    "    # optimizers for generator and discriminator\n",
    "    d_optimizer = optim.Adam(D.parameters(), lr)\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr)\n",
    " \n",
    "    # define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # get the training datasets\n",
    "    train_data = torch.load(r'D:\\engsci\\year 3\\CLASS\\APS360\\Project\\data\\one_hot\\Notes_Dataset_test_onehot_ws100.pt')\n",
    "\n",
    "    # prepare data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # keep track of loss and generated, \"fake\" samples\n",
    "    samples = []\n",
    "    losses = []\n",
    "\n",
    "    # fixed data for testing\n",
    "    sample_size=16 #Arbitrary.\n",
    "    test_noise = np.random.uniform(-1, 1, size=(sample_size, rand_size))\n",
    "    test_noise = torch.from_numpy(test_noise).float()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        D.train() #To enable dropouts...\n",
    "        G.train()\n",
    "        \n",
    "        for batch_i,real_samples in enumerate(train_loader):\n",
    "#             print(real_samples.shape)\n",
    "#             print(real_samples.view(-1, sample_length*16).shape)\n",
    "            batch_size = batch_size\n",
    "            real_samples = torch.tensor(real_samples).float()\n",
    "            \n",
    "            # === Train the Discriminator ===\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "#Note: calculate losses on real and fake images independently...\n",
    "            # discriminator losses on real images \n",
    "            D_real = D(real_samples)\n",
    "            labels = torch.ones(len(D_real),1)\n",
    "#             print('labels.shape: ',labels.shape)\n",
    "#             print('D_real.shape: ',D_real.shape)\n",
    "            d_real_loss = criterion(D_real, labels)\n",
    "            \n",
    "            # discriminator losses on fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            fake_samples = G(z)\n",
    "\n",
    "            D_fake = D(fake_samples)\n",
    "            labels = torch.zeros(len(D_fake),1) # fake labels = 0\n",
    "#             print('labels.shape: ',labels.shape)\n",
    "#             print('D_fake.shape: ',D_fake.shape)\n",
    "            d_fake_loss = criterion(D_fake, labels)\n",
    "            \n",
    "            # add up losses and update parameters\n",
    "            d_loss = d_real_loss + d_fake_loss #Add the two losses... Done separately to have the labels easily...\n",
    "            d_loss.backward() #Backpropagate (computes gradients, right?)\n",
    "            d_optimizer.step() #Take a step.\n",
    "            \n",
    "\n",
    "            # === Train the Generator ===\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # generator losses on fake song samples...\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, rand_size)) #Initial noise... Why -1 --> 1?? Because that's what tanh is... But pixel range still 0-->1... so...???\n",
    "            #Can change from uniform to gaussian if you want...\n",
    "            z = torch.from_numpy(z).float()\n",
    "            fake_samples = G(z) #Collect fake images generated. Somehow gets 0-->1 range... How...\n",
    "          \n",
    "            D_fake = D(fake_samples) \n",
    "            labels = torch.ones(len(D_fake),1) #flipped labels\n",
    "#             print('labels.shape: ',labels.shape)\n",
    "#             print('D_fake.shape: ',D_fake.shape)\n",
    "            # compute loss and update parameters\n",
    "            g_loss = criterion(D_fake, labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "#Essentially training two models.\n",
    "        # print loss\n",
    "        print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
    "              % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "        # append discriminator loss and generator loss\n",
    "        losses.append((d_loss.item(), g_loss.item()))\n",
    "        \n",
    "        # plot images\n",
    "        G.eval() #Set to evaluation mode, no longer training... Must implement this for the project if we want dropout to play into it.\n",
    "        D.eval()\n",
    "        test_images = G(test_noise)\n",
    "\n",
    "#         plt.figure(figsize=(9, 3))\n",
    "#         for k in range(16): #Generates a batch of 16...\n",
    "#             plt.subplot(2, 8, k+1)\n",
    "#             plt.imshow(test_images[k,:].data.numpy().reshape(28, 28), cmap='Greys')\n",
    "#         plt.show()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anacondainstal\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], d_loss: 6.2436, g_loss: 0.5225, \n",
      "Epoch [2/40], d_loss: 9.8859, g_loss: 0.0032, \n",
      "Epoch [3/40], d_loss: 1.6164, g_loss: 2.0687, \n",
      "Epoch [4/40], d_loss: 1.0381, g_loss: 4.1645, \n",
      "Epoch [5/40], d_loss: 0.7382, g_loss: 3.0895, \n",
      "Epoch [6/40], d_loss: 0.2629, g_loss: 2.6413, \n",
      "Epoch [7/40], d_loss: 0.0802, g_loss: 3.4489, \n",
      "Epoch [8/40], d_loss: 0.0384, g_loss: 4.2438, \n",
      "Epoch [9/40], d_loss: 0.0075, g_loss: 5.0493, \n",
      "Epoch [10/40], d_loss: 0.0277, g_loss: 5.2659, \n",
      "Epoch [11/40], d_loss: 0.0196, g_loss: 5.5687, \n",
      "Epoch [12/40], d_loss: 0.0034, g_loss: 6.0232, \n",
      "Epoch [13/40], d_loss: 0.0024, g_loss: 6.1818, \n",
      "Epoch [14/40], d_loss: 0.0020, g_loss: 6.3157, \n",
      "Epoch [15/40], d_loss: 0.0016, g_loss: 6.4579, \n",
      "Epoch [16/40], d_loss: 0.0015, g_loss: 6.7055, \n",
      "Epoch [17/40], d_loss: 0.0012, g_loss: 6.8233, \n",
      "Epoch [18/40], d_loss: 0.0011, g_loss: 6.9452, \n",
      "Epoch [19/40], d_loss: 0.0010, g_loss: 7.0420, \n",
      "Epoch [20/40], d_loss: 0.0036, g_loss: 7.1042, \n",
      "Epoch [21/40], d_loss: 0.0012, g_loss: 6.9015, \n",
      "Epoch [22/40], d_loss: 0.0011, g_loss: 6.7974, \n",
      "Epoch [23/40], d_loss: 0.0014, g_loss: 6.9585, \n",
      "Epoch [24/40], d_loss: 0.0010, g_loss: 7.1139, \n",
      "Epoch [25/40], d_loss: 0.0007, g_loss: 7.3133, \n",
      "Epoch [26/40], d_loss: 0.0006, g_loss: 7.4912, \n",
      "Epoch [27/40], d_loss: 0.0006, g_loss: 7.4380, \n",
      "Epoch [28/40], d_loss: 0.0007, g_loss: 7.5385, \n",
      "Epoch [29/40], d_loss: 0.0007, g_loss: 7.4532, \n",
      "Epoch [30/40], d_loss: 0.0006, g_loss: 7.6020, \n",
      "Epoch [31/40], d_loss: 0.0005, g_loss: 7.7788, \n",
      "Epoch [32/40], d_loss: 0.0004, g_loss: 7.8264, \n",
      "Epoch [33/40], d_loss: 0.0005, g_loss: 7.8927, \n",
      "Epoch [34/40], d_loss: 0.0006, g_loss: 8.0268, \n",
      "Epoch [35/40], d_loss: 0.0021, g_loss: 8.1137, \n",
      "Epoch [36/40], d_loss: 0.0004, g_loss: 8.1999, \n",
      "Epoch [37/40], d_loss: 0.0004, g_loss: 8.0085, \n",
      "Epoch [38/40], d_loss: 0.0023, g_loss: 7.3289, \n",
      "Epoch [39/40], d_loss: 0.0786, g_loss: 4.3422, \n",
      "Epoch [40/40], d_loss: 0.0069, g_loss: 5.9698, \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n@Eric Liu:\\n\\nPotential architecture:\\ninput = random noise of size (batch, number of notes, 16)\\nRNN --> LSTM or GRU of varying length and varying hidden layers\\nFCNN(s)\\noutput --> Size of (batch,number of notes,16). Reshape the FCNN output to match this or leave out FCNN and just use GRUs\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rand_size = 100; #arbitrary...\n",
    "\n",
    "D = Discriminator()\n",
    "G = Generator()\n",
    "\n",
    "losses = train(G, D, lr=0.02, batch_size=64, num_epochs=40) #Currently, discriminator is too good...\n",
    "#Need to build a better Generator architecture... Can start with RNN layer, feed it random noise and then go into some\n",
    "#fully connected layers?\n",
    "'''\n",
    "@Eric Liu:\n",
    "\n",
    "Potential architecture:\n",
    "input = random noise of size (batch, number of notes, 16)\n",
    "RNN --> LSTM or GRU of varying length and varying hidden layers\n",
    "FCNN(s)\n",
    "output --> Size of (batch,number of notes,16). Reshape the FCNN output to match this or leave out FCNN and just use GRUs\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl_jGnOd83Cj"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwvgvhzz83Cj"
   },
   "source": [
    "Since our model is 'tested' with people listening to it, we need to just generate some samples.\n",
    "To do so, and because this is a GAN, do as follows:\n",
    "\n",
    "TO DO: implement a note rounding mechanism to we ensure we always get valid notes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sample: (Watch for the same notes appearing...) [[[0. 0. 1. ... 1. 0. 0.]\n",
      "  [1. 0. 1. ... 1. 0. 0.]\n",
      "  [0. 0. 1. ... 1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]]]\n",
      "(100, 16)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "data byte must be in range 0..127",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b7965ca2338c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnew_excerpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_excerpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNumpy2Midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_excerpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test_from_model_68_GAN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-5b8bb5e41847>\u001b[0m in \u001b[0;36mNumpy2Midi\u001b[1;34m(notes, name)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0moff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNumpyAddOffNotes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0munseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNumpyUnsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mmid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNumpy2MidiDirect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mmid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".midi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5b8bb5e41847>\u001b[0m in \u001b[0;36mNumpy2MidiDirect\u001b[1;34m(array)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnote\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m         \u001b[1;31m# Get the index and the note. Array must be int array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtrack1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'note_on'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvelocity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Add the note to the track.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mmid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacondainstal\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\mido\\messages\\messages.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, type, **args)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sysex'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mmsgdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSysexData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert_py2_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mcheck_msgdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacondainstal\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\mido\\messages\\checks.py\u001b[0m in \u001b[0;36mcheck_msgdict\u001b[1;34m(msgdict)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 '{} message has no attribute {}'.format(spec['type'], name))\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mcheck_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anacondainstal\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\mido\\messages\\checks.py\u001b[0m in \u001b[0;36mcheck_value\u001b[1;34m(name, value)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcheck_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0m_CHECKS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacondainstal\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\mido\\messages\\checks.py\u001b[0m in \u001b[0;36mcheck_data_byte\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data byte must be int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m127\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data byte must be in range 0..127'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: data byte must be in range 0..127"
     ]
    }
   ],
   "source": [
    "number_of_samples = 1\n",
    "test_noise = np.random.uniform(-1, 1, size=(number_of_samples, rand_size))\n",
    "test_noise = torch.from_numpy(test_noise).float()\n",
    "test_samples = G(test_noise)\n",
    "test_samples = test_samples.detach().numpy()\n",
    "print('test_sample: (Watch for the same notes appearing...)',test_samples)\n",
    "new_excerpt = np.array(test_samples[0])\n",
    "print(new_excerpt.shape)\n",
    "mid = Numpy2Midi(np.array(new_excerpt),'test_from_model_68_GAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project_workbook_backup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
